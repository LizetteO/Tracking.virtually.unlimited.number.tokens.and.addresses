---

title: Commit rate management with decoupled commit operations
abstract: Consistency is managed among data operations by coalescing commit operations, adjusting a commit rate to optimize latency, and converging to fairness across servers. Write operations representing a change to data stored by one or more of a plurality of computing devices are received in succession by, for example, a cloud computing service. A state of a plurality of tracking objects is adjusted to indicate whether the change in data has been provided to the computing devices and whether the change in data has been acknowledged by the computing devices. Changes to the same data are coalesced such that only the most recent change is provided to the computing devices for storage. In some embodiments, the commit rate is adjustable such that the commit rate decreases quickly but increases slowly based on measured latencies.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08140478&OS=08140478&RS=08140478
owner: Microsoft Corporation
number: 08140478
owner_city: Redmond
owner_country: US
publication_date: 20090129
---
Typical cloud computing systems include front end servers middle tier servers and backend storage servers. Some existing services focus on addressing partitioning and recovery between the front end servers and the middle tier servers. Other services are developed for execution by the middle tier servers. To maintain consistency among data operations application developers implement logic for execution at the backend storage servers e.g. as structured query language instructions . Such logic however is difficult to program and separates implementation of the existing services across both the backend storage servers and the middle tier servers. For example application program developers create logic for assigning requests to the middle tier servers providing consistency semantics on the middle tier state communicating with the backend storage servers and calling any stored procedures at the backend storage servers appropriately.

Embodiments of the invention decouple commit operations from write operations to provide consistency and optimized latency. A plurality of tracking objects representing commit operations to be performed by one or more computing devices are accessed. A commit rate for the commit operations is defined. The accessed plurality of tracking objects are provided to the computing devices at the commit rate and a latency associated with the commit operations is measured. The measured latency is compared to the defined commit rate. The defined commit rate is adjusted based on the comparison and based on a factor determined relative to the defined commit rate.

This Summary is provided to introduce a selection of concepts in a simplified form that are further described below in the Detailed Description. This Summary is not intended to identify key features or essential features of the claimed subject matter nor is it intended to be used as an aid in determining the scope of the claimed subject matter.

Referring to the figures embodiments of the disclosure provide at least strong consistency including a formalization of semantics provided by data and operations in a distributed system such as a cloud service or other cloud computing system. Further the disclosure provides a programming model for building consistent application logic within the cloud service to allow opportunities for optimizing access to storage systems. Use of a generic scale out store is optimized with a dynamic control loop for low latency subject to fairness. The dynamic control loop is based on for example latency rather than explicit feedback from the cloud service. In some embodiments the disclosure is operable with any generic scale out store that supports at least basic Put and Get operations.

While described in the context of the cloud service in some embodiments the disclosure is applicable in embodiments other than the cloud service. For example the disclosure is applicable in embodiments in which a single computing device performs read and write operations on a memory area.

Referring again to an exemplary block diagram illustrates a typical configuration of a cloud service providing computing capabilities. One or more application programs executing on a client computer provide operations and data to the cloud service. The cloud service represents any service that receives requests e.g. from the application programs performs operations and responds to the requests. The exemplary cloud service in includes three tiers one or more front end servers such as front end server through front end server N one or more middle tier servers such as middle tier server through middle tier server M and one or more storage backend servers such as storage backend server through storage backend server P. The front end servers perform lightweight tasks such as user authentication and communication with users and the application programs . The front end servers query the middle tier servers for heavyweight operations. The middle tier servers execute the bulk of the functionality of the cloud service. The storage backend servers are responsible for data storage and other related operations.

Referring next to an exemplary block diagram illustrates a computing device for processing tracking objects representing changes to data. The computing device is for example part of the middle tier servers and manages dependencies among data operations in the cloud service by in part decoupling commit operations from write operations. In some embodiments the instructions and components stored in a memory area are implemented as one or more libraries that are linked in by all the middle tier servers . Communication with the storage backend servers is performed via application programming interfaces APIs included in the linked libraries. The logic executed by the computing device represents a scale out soft state middle tier that uses a hard state scale out backend to implement persistence.

The computing device includes a processor and the memory area . The processor is programmed to execute computer executable instructions for implementing aspects of the disclosure. In some embodiments the processor is programmed to execute instructions such as those illustrated in the figures e.g. .

The memory area or other computer readable media stores one or more tracking objects such as tracking object through tracking object S. The tracking objects track read or write operations. In some embodiments one or more of the tracking objects stored in the memory area correspond to one or more write operations to effect changes in data stored by a computing device such as any of the storage backend servers . In general the tracking objects have a one to many relationship with read and write actions. Each of the tracking objects has a key and a state . The key identifies the tracking object while the state indicates whether the change in data has been provided to the storage backend servers and whether the change in data has been acknowledged by the storage backend servers .

In some embodiments there is a single owner of each key . For example the presence data for a single messaging user may be stored under a single partition key e.g. the electronic mail address of the user . Aspects of the disclosure provide consistency at the granularity of the key and commit data at either the key or sub key level e.g. for each field within the presence data to reduce commit overhead .

The state of each of the tracking objects may be implemented as for example two bits and a queue of callbacks. The two bits include a dirty bit and an outstanding bit. The queue of callbacks is specified by the application in the middle tier that is using memory area . In some embodiments the callbacks correspond to sending messages that reflect the successful completion of the operation. The dirty true value means there are changes that have not yet been sent to the storage backend servers . The outstanding true value means there are changes that have been sent to the storage backend servers but not yet acknowledged. It is safe to execute a callback immediately if dirty false and outstanding false . If dirty false but outstanding true the callback is added to a first in first out FIFO queue of callbacks to execute when the commit returns from the storage backend servers . If dirty true and regardless of what the outstanding bit is the callback is added to a FIFO queue that is waiting on serialization of this dirty object. After the dirty object is serialized and sent to the storage backend servers the entire queue of callbacks associated with that object waits on the commit returning from the storage backend servers . This logic handles both callbacks associated with a read and callbacks associated with a write. The write caused the object to be marked dirty before the callback was enqueued.

The memory area further stores one or more computer executable components. The components include for example a persistence component an interface component and a dependency component . These components are described below with reference to .

In the example of the memory area is within the computing device . However the memory area or any of the data stored thereon may be associated with any server or other computer local or remote from the computing device e.g. accessible via a network .

Referring next to an exemplary block diagram illustrates the serialization and commitment of data operations. A controller requests that the application program serialize the data operations. The controller algorithmically determines when to make these requests so as to reduce messaging overhead to the persistent store . In an example the controller includes logic executing on one or more of the middle tier servers . The application program serializes the data operations and provides the serialized data operations in for example a single communication to the controller .

The controller marks the serialized data operations as dirty deleted or otherwise changed. The time trepresents the time for serialization. The controller provides the data operations e.g. commit or read operations to a persistent store e.g. one or more of the storage backend servers . The persistent store performs the data operations and provides an acknowledgement to the controller . The time trepresents the time for reads and writes to the persistent store .

In an embodiment the persistence component the interface component and the dependency component execute as part of the controller . The persistence component manages a commit rate for the commit operations based in part on the performance of the commit operations. The interface component e.g. a storage proxy accesses a plurality of the tracking objects e.g. received in succession from the application programs in some embodiments . The dependency component alters responsive to the accessing by the interface component the state of the tracking objects to indicate that the change in data has not been provided to the persistent store . For example the received tracking objects are marked as dirty. The dependency component further combines or coalesces the operations tracked by the tracking objects corresponding to the same data such that the dependency component provides the change in data from only the last received of the combined tracked operations to the persistent store at the commit rate managed by the persistence component . Coalescing is further described below with reference to .

In an example in which the controller executes on a plurality of the middle tier servers in the cloud service the persistence component executes to adjust a commit rate of each of the middle tier servers such that the commit rates of the middle tier servers converge over time.

In some embodiments the data operations are tracked by the tracking objects and the controller coalesces the data operations to reduce the quantity of write operations affecting the same data stored by the persistent store . In operation the controller receives one or more data operations that are writes from the middle tier program or a plurality of the middle tier programs in succession during a predefined interval. The controller marks the tracking objects as dirty by altering the state of each of the tracking objects to indicate that the change in data tracked by the tracking objects has not been provided to the persistent store . The controller identifies a plurality of tracked data operations as having the same key e.g. affecting the same data in the persistent store . The controller communicates with the persistent store to commit the change in data corresponding only to the data operation received last during the predefined interval. The controller alters the state of the tracking object to indicate that the change in data has been provided to the persistent store . After storage the persistent store notifies the controller that the change in data has been stored. The controller notifies the middle tier program corresponding to the identified tracking object of the committed change by executing the callbacks that the middle tier program had earlier specified.

A graphical illustration of the coalescing of data operations is next shown in following by illustrations of dependency management in and .

Referring next to an exemplary sequence diagram illustrates the coalescing of write operations during a time interval. The data operation Write k a first received by the middle tier program in this example is provided to the middle tier. The middle tier marks the data associated with key k as dirty. Before committing the write operation e.g. a commit interval has not elapsed yet two additional data operations are received by the middle tier program Write k b and Write k c . The data associated with key k is still dirty and there have been no commit operations since before the first write operation was received. After the commit interval has elapsed the middle tier commits only the last received write operation for the data associated with key k. Writing c into the data block associated with key k is semantically equivalent to writing a then b and then c . However by coalescing the write operations by only writing c the middle tier has reduced the quantity of the write operations performed. The storage backend servers then acknowledge all three of the write operations.

The amount of coalescing and batching of data operations is adjustable by for example a control loop such as described below in . The dynamic control loop optimizes end to end latency to avoid overloading the storage backend servers when the workload is intense and avoids underperforming when the storage backend servers are lightly loaded. For a given workload committing less frequently results in higher coalescing and batching but decreases the responsiveness.

Referring next to an exemplary sequence diagram illustrates performance of a read operation on data marked as clean. In the example of a read operation Read k is received by the middle tier program. A component within the middle tier program confirms that the data associated with key k is clean with no outstanding commit operations and executes a callback to the middle tier immediately. The middle tier then releases the data associated with k to the front end server .

Referring next to an exemplary sequence diagram illustrates performance of a read operation on data marked as dirty. In the example of a read operation Read k is received by the middle tier program. The component in the middle tier determines that the data associated with key k is dirty whether or not there are outstanding commit operations which means that a change to the data associated with k has been received by the middle tier but not yet provided to the storage backend servers . As a result the middle tier waits for the commit operation to occur and then provides the data associated with k to the front end server after the storage backend servers acknowledge the commit operation.

Referring next to an exemplary sequence diagram illustrates performance of a read operation on data marked as clean but at least one outstanding commit operation associated therewith. In the example of a read operation Read k is received by the middle tier program. The middle tier determines that the data associated with key k is clean but that there is an outstanding commit operation. The middle tier waits for acknowledgement of the commit operation from the storage backend servers before releasing the data associated with k to the front end server .

In the examples of and there is a latency associated with the commit operations. In some embodiments the latency is measured and used to adjust a commit rate of the middle tier servers as described next with reference to .

Referring next to an exemplary flow chart illustrates the dynamic adjustment of a rate for the computing device to perform commit operations. In some embodiments the computing device is part of the middle tier servers . A plurality of the tracking objects are created for example in response to calls made by one of the middle tier servers . At a commit interval is defined. The commit interval corresponds to a commit rate or rate at which the middle tier server commits the data operations to the storage backend servers . The available items are committed at each interval. The data operations are sent by the middle tier server to the storage backend servers in accordance with the commit rate. At a latency associated with performance of the commit operations is measured. For example the latency may be associated with the mean or median of the measured latencies of a plurality of commit operations. The latency is used as an indirect measurement of load. Generally the measured latency may reflect the size of the request network congestion application queuing and actual execution latency. In some embodiments the latency includes the time spent providing the data operations to the storage backend servers and the time spent waiting for an acknowledgement from the storage backend servers of the commit operations. In an example the interval for measuring latency is proportional to a recently measured latency.

At the measured latency is compared with the defined commit interval. The commit rate or interval is adjusted at and responsive to the comparison at . If the measured latency exceeds the defined commit interval at the commit interval is increased at . If the measured latency does not exceed the defined commit interval at the commit interval is decreased at .

The commit interval is adjusted by a factor relative to the existing commit interval. Aspects of the disclosure optimize response time by slowly increasing the commit rate when appropriate to stay within an optimized range or state longer while decreasing the commit rate quickly but not drastically to move back within the optimized range or state. This self tuning of the commit rate accommodates for possibly large queues of work sitting at the storage backend servers .

Furthermore to provide fairness among the multiple servers if one of the middle tier servers measures an increase in response time and the logic in indicates that the commit interval be increased and the middle tier had previously measured a slow response time the commit interval is increased by an amount smaller than if the middle tier server had previously measured a fast response time. Similarly if the one of the middle tier servers measures a decrease in response time and the logic in indicates that the commit interval be decreased and the middle tier had previously measured a slow response time the commit interval is decreased by an amount greater than if the middle tier server had previously measured a fast response time.

As an example if given a maximum latency or commit interval and a minimum latency or commit interval the factor is determined based on the maximum latency the minimum latency and the existing commit interval such as shown below in Equation 1 . factor 1 ratio maximumLatency commitInterval maximumLatency minimumLatency 1 The ratio represents a default factor for adjusting the commit interval.

Referring next to an exemplary graph illustrates rate convergence for commit operations performed by two computing devices. In a cloud computing embodiment the two computing devices represent middle tier servers and execute the logic illustrated in . The vertical axis shows delay in milliseconds and the horizontal axis shows elapsed time in seconds. In the example of the service time e.g. latency changes from 10 milliseconds to 20 milliseconds at 150 seconds and read or write requests are generated every two milliseconds. In an embodiment the optimal delay equals the quantity of middle tier servers times the service time. The two lines in the graph illustrate the convergence of the delay of the two middle tier servers over time. The convergence of the delay represented by the two lines shows that the workload is being balanced between the two middle tier servers which provides consistent responsiveness from the middle tier.

Exemplary logic for execution by the application programs calling interfaces of the disclosure is shown below.

Exemplary logic for the application programs to demand load state to the middle tier from the backend is shown below.

Exemplary pseudo code and examples of the control flow of the persistence component is shown in Appendix B.

While aspects of the invention are described with reference to the computing device embodiments of the invention are operable with any computing device. For example aspects of the invention are operable with devices such as laptop computers gaming consoles including handheld gaming consoles hand held or vehicle mounted navigation devices portable music players a personal digital assistant an information appliance a personal communicator a handheld television or any other type of electronic device.

By way of example and not limitation computer readable media comprise computer storage media and communication media. Computer storage media store information such as computer readable instructions data structures program modules or other data. Communication media typically embody computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and include any information delivery media. Combinations of any of the above are also included within the scope of computer readable media.

Although described in connection with an exemplary computing system environment embodiments of the invention are operational with numerous other general purpose or special purpose computing system environments or configurations. Examples of well known computing systems environments and or configurations that may be suitable for use with aspects of the invention include but are not limited to mobile computing devices personal computers server computers hand held or laptop devices multiprocessor systems gaming consoles microprocessor based systems set top boxes programmable consumer electronics mobile telephones network PCs minicomputers mainframe computers distributed computing environments that include any of the above systems or devices and the like.

Embodiments of the invention may be described in the general context of computer executable instructions such as program modules executed by one or more computers or other devices. The computer executable instructions may be organized into one or more computer executable components or modules. Generally program modules include but are not limited to routines programs objects components and data structures that perform particular tasks or implement particular abstract data types. Aspects of the invention may be implemented with any number and organization of such components or modules. For example aspects of the invention are not limited to the specific computer executable instructions or the specific components or modules illustrated in the figures and described herein. Other embodiments of the invention may include different computer executable instructions or components having more or less functionality than illustrated and described herein.

Aspects of the invention transform a general purpose computer into a special purpose computing device when configured to execute the instructions described herein.

The embodiments illustrated and described herein as well as embodiments not specifically described herein but within the scope of aspects of the invention constitute exemplary means for managing dependencies among data operations at a middle tier in a cloud service and exemplary means for optimizing a latency of commit operations for the tracking objects by adjusting a commit rate differentially.

The order of execution or performance of the operations in embodiments of the invention illustrated and described herein is not essential unless otherwise specified. That is the operations may be performed in any order unless otherwise specified and embodiments of the invention may include additional or fewer operations than those disclosed herein. For example it is contemplated that executing or performing a particular operation before contemporaneously with or after another operation is within the scope of aspects of the invention.

When introducing elements of aspects of the invention or the embodiments thereof the articles a an the and said are intended to mean that there are one or more of the elements. The terms comprising including and having are intended to be inclusive and mean that there may be additional elements other than the listed elements.

Having described aspects of the invention in detail it will be apparent that modifications and variations are possible without departing from the scope of aspects of the invention as defined in the appended claims. As various changes could be made in the above constructions products and methods without departing from the scope of aspects of the invention it is intended that all matter contained in the above description and shown in the accompanying drawings shall be interpreted as illustrative and not in a limiting sense.

Initialize hysteresis serviceLatencyFrac increaseRatio decreaseRatio ceiling floor unhappinessThreshold minSampleCount.

 At the beginning of every measurement interval there may be new values for commitInterval ti and tbest.

At the end of each measurement interval update ti to be the average latency of requests to the store that completed during this measurement interval.

The store is responding very well and suddenly gets slower e.g. tbest 40 milliseconds ti 80 milliseconds at the end of a measurement interval. In this case ti 1.3 tbest evaluates to true and the UNHAPPY case occurs. The following values are then set 

The commitInterval is now longer by a factor of about 1.4. This corresponds to backing off because the store is busy.

The store is responding moderately well and suddenly gets faster e.g. tbest 40 milliseconds ti 30 milliseconds. The store performance additionally stayed at this new good level for a number of measurement intervals. In this case ti 1.3 tbest consistently evaluates to false and the HAPPY case is executed. The following values are then set 

The commitInterval is now shorter which corresponds to being more aggressive at using the store because the store is underloaded.

Referring again to the first example above suppose the commitInterval 500 milliseconds e.g. the commitInterval was much slower to begin with . In this case when the UNHAPPY state is entered the following values are assigned.

The commitInterval is longer by a factor of only 1.3. As in the first example this corresponds to backing off because the store is busy. However the increase of 1.3 is less than the earlier increase of 1.4. This illustrates the relatively slower store the one with commitInterval 500 backing off more slowly than the faster store the one with commitInterval 10 . This slower relative backoff enables converging to fairness.

