---

title: Organizing and managing a memory blade with super pages and buffers
abstract: A system and method is illustrated wherein a protocol agent module receives a memory request encoded with a protocol, the memory request identifying an address location in a memory module managed by a buffer. Additionally, the system and method includes a memory controller to process the memory request to identify the buffer that manages the address location in the memory module. Further, the system and method includes an address mapping module to process the memory request to identify at least one super page associated with the memory module, the at least one super page associated with the address location.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08645610&OS=08645610&RS=08645610
owner: Hewlett-Packard Development Company, L.P.
number: 08645610
owner_city: Houston
owner_country: US
publication_date: 20090629
---
This is non provisional patent application is related to Patent Cooperation Treaty Application Number PCT US2008 069168 entitled MEMORY SERVER that was filed on Jul. 3 2008 and which is incorporated by reference in its entirety.

Repeater buffers are used in the facilitation of data reads and writes from memory. Additionally these repeater buffers are dedicated to the management a particular type of memory. Further repeater buffers are used by computer systems in the management of virtual memory to conduct the above referenced reads and writes.

A system and method is illustrated for facilitating memory management of a plurality of disparate memory devices using one or more repeater buffers. These repeater buffers referenced herein as buffers reside upon a memory blade. Memory management includes the allocation or de allocation of virtual memory pages utilized by compute blades. Virtual memory pages as referenced herein include super pages comprised of sub pages. A memory blade is a device for managing a plurality of memory devices that may be utilized by a compute blade. A compute blade as referenced herein is a computer system with memory to read input commands and data and a processor to perform commands manipulating that data. Example memory devices referenced herein as memory modules include Static Random Access Memory SRAM Dynamic Random Access Memory DRAM Electrically Erasable Programmable Read Only Memory EEPROM referenced herein as flash memory other Main Memory implementation e.g. magnetic flash or optically based memory or other suitable memory. A buffer as used herein is a device used to temporarily store and process e.g. encode and decode data for storage into one or more of the memory devices.

In some example embodiments the virtual memory pages are managed using a system of mapped addresses. For example a blade identifier bladeID and System Machine Address SMA are used by the memory blade to map to a Remote Memory Machine Address RMMA . Specifically a map register is indexed using the bladeID where each entry in the map register e.g. a base entry and a limit entry represents the number of super pages managed by the memory blade identified using the bladeID. Further the base entry and a super page ID parsed from the SMA are used to index into an RMMA map structure. Each entry in the RMMA map represents a super page and the permissions associated with this super page. A super page is a virtual memory page of for example 16 KB or larger. A sub page is a virtual memory page that is for example smaller than 16 KB.

In some example embodiments a system and method is illustrated for a memory blade design that provides a unified command interface e.g. Application Programming Interfaces APIs for multiple access granularities and usage models and using memory modules with heterogeneous density and timing characteristics and modular composition of multiple memory blades. The API facilitates read and write functionality for the memory blade. Additionally the API allows for memory on the memory blade to be allocated or de allocated by a compute blade. Additionally the API allows for address mappings and permissions to be retrieved and set. This API also allows a single memory blade to be shared by multiple compute blades.

In some example embodiments multiple on board repeater buffers through are implemented with each buffer acting as an independent memory controller or manager for a subset of the memory modules. As illustrated in a single buffer may be used to manage 2 4 memory modules. The management duties for a buffer e.g. buffers through include receiving memory operation commands and data from a central memory controller e.g. central memory controller and forwarding the data to individual memory modules e.g. memory modules through . Responses are sent from one more of the memory modules through via the buffers through to the central memory controller . Additionally as will be more fully illustrated below a buffer may store and use a particular codec to format data for storage in a particular memory module to which the buffer is operatively connected.

In some example embodiments various reliability and serviceability optimizations are implemented for the buffers. For example in cases where the memory blade maintains a free list of super pages memory scrubbing can be initiated for allocated virtual memory pages in the background to avoid unnecessary failures. Scrubbing as referenced herein includes the detection and recovery of virtual memory pages that include data errors. Further because memory allocation information is maintained on the memory blade the memory module can map out memory modules e.g. DRAM for which a fault is detected and use memory modules for which a fault has not been detected. Additionally memory provisioning repair and upgrade can be done on a single memory blade without having to modify individual compute blades. Further the buffer and can also include an indicator Light Emitting Diode LED light to indicate the operational status of these memory modules.

In some example embodiments the call sequence for the various tables maintained by the address mapping module is illustrated as follows. Accessing data on the remote memory involves address mapping and performing a read or write to a memory blade and memory maintained therein. In one example embodiment memory is organized and managed in large chunks e.g. as a plurality of 16 MB super pages so that one single entry in the RMMA mapping table corresponds to thousands of contiguous operating system pages e.g. virtual memory pages . These virtual memory pages are accessed based upon the needs of the compute blades and the applications residing therein.

Some example embodiment include the use of the above referenced map registers to facilitate RMMA partitioning to identify particular compute blades that are to receive a memory allocation or that are to have memory de allocated. An RMMA may be partitioned in a particular manner to enforce a static or dynamic memory allocation. Under a static partitioning regime each compute blade is assigned a fixed portion of remote memory. This remote memory is allocated at startup time and remains constant during the compute blade s uptime. Under a dynamic partitioning regime each compute blade is given an initial allocation of remote memory and can request additional memory as application demand changes. In some example embodiments the memory blade orchestrates memory reallocation by revoking allocations from some compute blades e.g. compute blade to provide capacity to others e.g. compute blade . Such changes are recorded as the map register values to reflect the capacity allocation for each compute blade.

In certain cases RMMA sharing may be implemented by a compute blade. An owner of a RMMA region is defined as the compute blade to which the RMMA region is allocated. This owner can choose to map the RMMA into another blade s SMA therefore allowing the data stored in this region to be shared among both blades e.g. compute blade and . This is achieved by the mapping and sharing interface illustrated below.

In example cases where decisional operation evaluates to true an operation is executed. Operation is executed to processes the read request using the protocol agent . An operation is executed to parse the address of the data from the read request . Specifically the read request is unpacked or parsed and separated from its protocol header. For example a SMA may be parsed to retrieve a super page address through the use of operation . Operation is executed to index into the RMMA map table see e.g. operation referenced above . A decisional operation is executed to determine whether an index within the RMMA map table has been identified. In cases where decisional operation evaluates to false an exception is thrown. This exception may be handled by the hypervisor . In cases where decisional operation evaluates to true a decisional operation is executed. Decisional operation when executed determines whether the appropriate permissions exist to access a particular RMMA map table entry. In cases where decisional operation evaluates to false an exception is thrown as denoted at . Exceptions thrown at are handled by the hypervisor . In example cases where decisional operation evaluates to true an operation is executed. Operation transmits or otherwise sends a read command to a buffer see e.g. buffer . Operation is executed to optionally power up and retrieve decoded data from memory such as DRAM or DRAM . This memory is represented generically as data store . Operation is executed to packetized data using the protocol agent . This packetization process includes affixing a header to the decoded data such that the decoded data can be transmitted across a memory channel such as memory channel or . This data is transmitted as a response message in the form of requested data . Operation is executed to optionally power down memory in the form of for example DRAM or . In some example embodiments in parallel with the read the ECC and dirty bits are fetched and packetized with the data as part of the response message for the read request .

In some example embodiments one or more optimizations of the memory blade are implemented. For example where data is to be read or written locally to the compute blade power optimization is facilitated through putting the DRAM on the memory blade into an active power down mode. A second example of power optimization is facilitated through placing data within consecutive virtual memory pages on the same DRAM residing on the memory blade . As to the first example of power optimization a powering down of the DRAM may occur whenever the idle period of the DRAM has been longer than a given threshold and the buffer e.g. buffer can command the DRAM into active power down mode. The buffer may receive a power down command from the compute blade via another buffer or the memory controller via a memory channel. A given threshold may be some unit of time. With respect to the second example of power optimization virtual memory pages are clustered to a reduced number of DRAM such that a fewer number of DRAM is utilized than would otherwise be used to store data. As illustrated above DRAM not in use is powered down.

In some example embodiments an API is provided for the memory blade so as to allow compute blades and other devices to interface with the memory blade. This API may be exposed by the central memory controller to a compute blade. These APIs provide an interface to utilize the data use modules referenced above. Illustrated are various data access functions. These data access functions include 

The read function takes three parameters that include a blade identifier bladeID value a blade machine address bladeMA value and a size value. A page of virtual memory is returned through the execution of this function as data. The bladeID identifies a particular memory blade from which data is being retrieved. Additionally the bladeMA value is a logical or physical address for a memory blade. The size value reflects the size of the virtual memory page being retrieved for reading. These various parameters may be integers or some other suitable value. Additionally as illustrated above the write function takes the three aforementioned parameters plus a data parameter that may denote the data type of the data being written to memory.

In some example embodiments the above referenced API includes various capacity management functions that include 

The capacity management functions allow for a compute blade to request that memory be allocated or de allocated e.g. released on the memory blade . Like the read function illustrated above both of the capacity management functions take for example three parameters e.g. bladeID bladeMA and size . These parameters however are used for a purpose distinct from that outlined above with respect to the data access functions. Here for example these three parameters are used to identify a particular blade upon which memory is to be allocated. Additionally the size of the virtual pages to be allocated is also provided.

Some example embodiments of the above referenced API include a mapping and sharing API. The mapping and sharing API has the following form 

The getmap function takes two parameters and returns an integer in the form of a Remote Memory Machine Address RMMA . These two parameters are the aforementioned bladeID and bladeMA. The setMap function takes four parameters that include bladeID and bladeMA RMMA and a permission value. The permission value may be a boolean value integer value character value or other suitable values used to denote a particular permission field in the RMMA map table .

In some example embodiments a VM is migrated from one compute blade to another compute blade such that the memory contents of the migrated compute blade needs to be copied. In cases where address mapping is implemented see e.g. RMMA Map on a memory blade only local memory for the VM need be migrated through the use of the above referenced setMap function.

In some example embodiments the operation is executed to facilitate ECC coding and capacity optimizations on the buffers and . The buffers can encode the ECC bits on groups of cache block granularity e.g. 64 bytes as compared to typical 8 byte granularity reducing ECC overhead due to encoding 12 bits per 64 bytes. This generates a resulting cost savings when ECC is performed.

In some example embodiments prior to or while executing the method various memory allocation and de allocation operations are performed by the central memory controller . These allocations and de allocations may be executed by the operation prior to the writing of data. In one example embodiment the memory blade maintains a list of free virtual super pages to track capacity for allocation and de allocation. In cases where the virtual free pages runs low in the memory blade the memory blade coordinates with the hypervisors e.g. hypervisor on individual compute blades e.g. compute blades and to inflate one or more VM memory balloon drivers e.g. balloon driver . The inflating of the VM memory balloon drivers frees pages in the VM address space for use by another compute blade.

The SATA port may interface with a persistent storage medium e.g. an optical storage devices or magnetic storage device that includes a machine readable medium on which is stored one or more sets of instructions and data structures e.g. software embodying or utilized by any one or more of the methodologies or functions illustrated herein. The software may also reside completely or at least partially within the SRAM and or within the CPU during execution thereof by the computer system . The instructions may further be transmitted or received over the 10 100 1000 ethernet port USB port or some other suitable port illustrated herein.

In some example embodiments a removable physical storage medium is shown to be a single medium and the term machine readable medium should be taken to include a single medium or multiple medium e.g. a centralized or distributed database and or associated caches and servers that store the one or more sets of instructions. The term machine readable medium shall also be taken to include any medium that is capable of storing encoding or carrying a set of instructions for execution by the machine and that cause the machine to perform any of the one or more of the methodologies illustrated herein. The term machine readable medium shall accordingly be taken to include but not be limited to solid state memories optical and magnetic medium and carrier wave signals.

Data and instructions of the software are stored in respective storage devices which are implemented as one or more computer readable or computer usable storage media or mediums. The storage media include different forms of memory including semiconductor memory devices such as DRAM or SRAM Erasable and Programmable Read Only Memories EPROMs Electrically Erasable and Programmable Read Only Memories EEPROMs and flash memories magnetic disks such as fixed floppy and removable disks other magnetic media including tape and optical media such as Compact Disks CDs or Digital Versatile Disks DVDs . Note that the instructions of the software discussed above can be provided on one computer readable or computer usable storage medium or alternatively can be provided on multiple computer readable or computer usable storage media distributed in a large system having possibly plural nodes. Such computer readable or computer usable storage medium or media is are considered to be part of an article or article of manufacture . An article or article of manufacture can refer to any manufactured single component or multiple components.

In the foregoing description numerous details are set forth to provide an understanding of the present invention. However it will be understood by those skilled in the art that the present invention may be practiced without these details. While the invention has been disclosed with respect to a limited number of embodiments those skilled in the art will appreciate numerous modifications and variations therefrom. It is intended that the appended claims cover such modifications and variations as fall within the true spirit and scope of the invention.

