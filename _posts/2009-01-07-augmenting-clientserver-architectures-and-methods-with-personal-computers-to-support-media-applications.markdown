---

title: Augmenting client-server architectures and methods with personal computers to support media applications
abstract: Systems and methods according to exemplary embodiments provide systems and methods for augmenting the capabilities of a client device. The client device can be augmented by a device which has additional processing and memory capability to perform additional functions such as, for example, the translation of desired media into a format usable by the client device.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=09100716&OS=09100716&RS=09100716
owner: HILLCREST LABORATORIES, INC.
number: 09100716
owner_city: Rockville
owner_country: US
publication_date: 20090107
---
This application is related to U.S. patent application Ser. No. 11 144 880 filed on Jun. 3 2005 entitled Client server Architectures and Methods for Zoomable User Interfaces the disclosure of which is incorporated here by reference. This application is related to and claims priority from U.S. Provisional Patent Application Ser. No. 61 010 226 filed on Jan. 7 2008 entitled Augmenting Client Server Architectures and Methods with Personal Computers to Support Media Applications the disclosure of which is incorporated here by reference.

The present invention describes systems and methods for processing and transferring multimedia data between nodes in a communication system e.g. an interactive television system usable to create for example sophisticated entertainment user interfaces in the home.

Technologies associated with the communication of information have evolved rapidly over the last several decades. Television cellular telephony the Internet and optical communication techniques to name just a few things combine to inundate consumers with available information and entertainment options. Taking television as an example the last three decades have seen the introduction of cable television service satellite television service pay per view movies and video on demand. Whereas television viewers of the 1960s could typically receive perhaps four or five over the air TV channels on their television sets today s TV watchers have the opportunity to select from hundreds and potentially thousands of channels of shows and information. Video on demand technology currently used primarily in hotels and the like provides the potential for in home entertainment selection from among thousands of movie titles. Digital video recording DVR equipment such as offered by TiVo Inc. 2160 Gold Street Alviso Calif. 95002 further expand the available choices.

The technological ability to provide so much information and content to end users provides both opportunities and challenges to system designers and service providers. One challenge is that while end users typically prefer having more choices rather than fewer this preference is counterweighted by their desire that the selection process be both fast and simple. Unfortunately the development of the systems and interfaces by which end users access media items has resulted in selection processes which are neither fast nor simple. Consider again the example of television programs. When television was in its infancy determining which program to watch was a relatively simple process primarily due to the small number of choices. One would consult a printed guide which was formatted for example as series of columns and rows which showed the correspondence between 1 nearby television channels 2 programs being transmitted on those channels and 3 date and time. The television was tuned to the desired channel by adjusting a tuner knob and the viewer watched the selected program. Later remote control devices were introduced that permitted viewers to tune the television from a distance. This addition to the user television interface created the phenomenon known as channel surfing whereby a viewer could rapidly view short segments being broadcast on a number of channels to quickly learn what programs were available at any given time.

Despite the fact that the number of channels and amount of viewable content has dramatically increased the generally available user interface and control device options and frameworks for televisions have not changed much over the last 30 years. Printed guides are still the most prevalent mechanism for conveying programming information. The multiple button remote control with simple up and down arrows is still the most prevalent channel content selection mechanism. The reaction of those who design and implement the TV user interface to the increase in available media content has been a straightforward extension of the existing selection procedures and interface objects. Thus the number of rows and columns in the printed guides has been increased to accommodate more channels. The number of buttons on the remote control devices has been increased to support additional functionality and content handling. However this approach has significantly increased both the time required for a viewer to review the available information and the complexity of actions required to implement a selection. Arguably the cumbersome nature of the existing interface has hampered commercial implementation of some services e.g. video on demand since consumers are resistant to new services that will add complexity to an interface that they view as already too slow and complex.

An exemplary control framework having a zoomable graphical user interface for organizing selecting and launching media items is described in U.S. patent application Ser. No. 10 768 432 filed on Jan. 30 2004 to Frank A. Hunleth the disclosure of which is incorporated here by reference. This framework provides exemplary solutions to the afore described problems of conventional interfaces. Among other things such exemplary frameworks provide mechanisms which display metadata associated with media items available for selection by a user in a manner which is easy to use but allows a large number of different media items to be accessible. One feature of exemplary frameworks described in this patent application is the use of zooming to provide among other things visually informative transitions between different semantic levels of media objects displayed by the interface and as a mechanism for highlighting objects currently being considered by a user.

The implementation of these types of advanced user interfaces is complicated by the system architectures and communication nodes involved in the processing and transport of data used to generate these interfaces from various sources to an end user s device e.g. a television. As will be described in more detail below this data includes so called metadata that describes the media content. The term metadata as it is used herein refers to all of the supplementary information that describes the particular content of interest associated with media items available for selection by a user. As an example for movie objects the metadata could include e.g. the title description genre cast DVD cover art price availability cast bios and filmographies links to similar movies critical reviews user reviews the rights associated with the metadata itself rights associated with the content advertising metadata linked to the content of interest etc. An exemplary system for capturing processing synthesizing and forwarding metadata suitable for such advanced user interfaces is described in U.S. patent application Ser. No. 11 037 897 entitled A Metadata Brokering Server and Method filed on Jan. 18 2005 the disclosure of which is incorporated here by reference.

Once captured and processed however the data needs to be communicated from for example a head end portion of the system to for example a set top box in a manner which enables sufficient data to be supplied to render rich user interfaces while at the same time being sensitive to time delay and operating within the constraints imposed by legacy hardware. Accordingly it would be desirable to provide architectures and methods which resolve these conflicting parameters and enable advanced user interfaces to be generated.

Systems and methods according to exemplary embodiments can improve service within the telecommunications field.

According to one exemplary embodiment a zoomable user interface system includes a display device for displaying the zoomable user interface a client device connected to the display device for receiving a command to zoom into the zoomable user interface and for transmitting a request to perform a function associated with the command and a second device connected to the client device for receiving the request performing the function and returning a result to the client device wherein the client device uses the result to perform the zoom into the zoomable user interface on the display device.

According to another exemplary embodiment a method for augmenting a client device includes receiving a request to perform at least one function processing the request to perform the at least one function performing the at least one function which results in a first output selectively translating the first output into a format usable by the client device into a second output and transmitting either the first output or the second output to the client device.

According to yet another exemplary embodiment a communications node for augmenting a client device includes a processor in conjunction with at least one software application for processing a request to perform at least one function wherein the processor performs the steps of performing the at least one function which results in a first output and selectively translating the first output into a format usable by the client device into a second output a memory for storing the at least one software application the first output and the second output and a communications interface for receiving the request to perform at least one function and for transmitting either the first output or the second output to the client device.

The following detailed description of the invention refers to the accompanying drawings. The same reference numbers in different drawings identify the same or similar elements. Also the following detailed description does not limit the invention. Instead the scope of the invention is defined by the appended claims.

In order to provide some context for this discussion exemplary user interface screens which can be created using data and instructions forwarded from a server to a client in accordance with exemplary embodiments of the present invention are shown in and . Therein a portion of an exemplary user interface screen which can be generated based on information transferred to an end user s system e.g. set top box television or personal computer shows ten media selection items. For more information regarding this purely exemplary interface including previous screens and navigation techniques the interested reader is directed to the above incorporated by reference U.S. patent application Ser. No. 10 768 432 as well as to U.S. patent application Ser. No. 11 437 215 entitled Global Navigation Objects in User Interfaces the disclosure of which is also incorporated here by reference. It will be appreciated that such user interfaces are purely exemplary and that architectures and methods in accordance with the present invention can be implemented to support other interfaces.

The interface screens shown in and are purely exemplary and metadata and other data transferred and processed in accordance with the present invention can be used to support other interfaces or for purposes other than interface generation. Likewise many different types of information can be received and processed in accordance with the present invention. Examples of metadata types sources and associated uses e.g. for a TV browser interface a video on demand VOD interface or a music browser are shown in the table of . Of particular interest for this detailed discussion are the zooming features associated with user interfaces generated in accordance with these exemplary embodiments of the present invention. Although the present invention is not limited to techniques or systems for generating zoomable user interfaces and in fact one exemplary embodiment described below supports other applications such as an Internet browser some of the client server features discussed herein are particularly beneficial for use in conjunction with user interfaces which include zooming transitions between user interface screens. For the purpose of this detailed description the terms zoom zoomable and zooming refer to techniques wherein a user interface action results in changes to the displayed portion of the user interface that a creates a change of perspective which is consistent and informative to the user. Zooming will typically include changes in object magnification e.g. camera style zooming but is expressly not limited thereto. For example another aspect of zooming in accordance with user interfaces is semantic zooming which includes the modification of a zoomed object in a manner which is independent of magnification e.g. the addition of text or a graphic to an object which was not present as part of the object at any level of magnification prior to the semantic zoom. For more information related to zoomable user interfaces the interested reader is referred to the above identified incorporated by reference patent application.

For context one example of a zooming transitions in accordance with exemplary embodiments of the present invention is the zooming transition between the user interface screen of and which involves a magnification change of a hoverzoomed object and optionally semantic zooming to that object as well. Another example is found in the transition between the user interface screen of and wherein the image associated with Apollo 13 has its magnification changed e.g. enlarged in relative to the similar image shown in and translated for use in . Panning effects can also be used to animate the zooming transition.

A general client server architecture for providing data processing and transport according to an exemplary embodiment of the present invention is shown in . Therein a user interface server communicates with a client device to generate a user interface on a display device in conjunction with inputs from for example a pointing device . Communication of data e.g. metadata and content data between the user interface server and the client device can involve any number of intermediate nodes not shown between the user interface server and the client device including hubs distribution servers and the like. Moreover some or all of the functional elements illustrated as being part of the user interface server can be located within one or more of these intermediate nodes or reside at the headend of the system . The display device can for example be a television a computer monitor display or any other display device. The client device can be embodied as a set top box a personal computer or any other device including a processing unit. The pointer can for example be a free space pointing device a mouse a remote control device a track ball a joystick or any other device capable of providing a pointing capability and can be connected to the client device either via wireline or wirelessly.

According to this exemplary embodiment of the present invention the server includes a transition and screen capturer an MPEG 2 transition and scene encoder an MPEG and ZSD cache a scene request processor and an MPEG stream transmitter which components operate to generate and manage the streaming of MPEG 2 data to client devices and to receive and respond to upstream requests from clients . The transition and screen capturer automates the gathering of scene data used to generate the user interface. At a high level this can be accomplished by navigating through e.g. a scene graph provided as input to the transition and screen capturer along with metadata and content and calling the MPEG 2 transition and scene encoder to generate MPEG 2 clips and scene description files associated with selected scenes to be displayed on display device . Detailed information associated with scene description files and formats also referred to herein as ZSD data according to exemplary embodiments of the present invention is provided below under the header Scene Description Data Format .

Navigation through the scene graph involves capturing and processing data associated with the various scenes which can be generated by the user interface. A scene as that term is used herein generally refers to the framework associated with any user interface screen which can be generated by the user interface which despite the sophisticated and dynamic nature of user interfaces in accordance with the present invention are all known a priori albeit at least some of the data used to populate the scenes will vary e.g. over time as content providers change for example metadata associated with their offerings. Thus although and show only portions of user interface screens each of those complete screens would be considered to be a scene. Table 1 below lists exemplary data which can be collected for each transition and Table 2 lists exemplary data for each scene 

The transition and scene capturer is thus able to acquire all of the information necessary to simulate all desired transitions in the user interface from for example a database not shown in which contains the complete user interface universe . The transition and scene capturer includes navigator controller and capture controller components which become active as a user generates inputs to the interface which command scene transitions. At a high level the navigation controller has the responsibility of navigation to and from every transition and scene. An exemplary navigation controller performs the following operations 1 obtain the next transition 2 navigate to the from scene 3 execute a focus command for this transition 4 notify the capture controller with the scene and transition information 5 execute the activation command 6 notify the capture controller when the animation completes 7 notify the capture controller with the scene and transition information reversed for the back transition 8 invoke a goBacko routine and 9 notify the capture controller when the animation completes.

The capture controller integrates with the MPEG 2 transition and scene encoder to create the MPEG 2 clips and ZSD files. The capture controller receives notifications from the navigation controller when the transition begins and ends and invokes routines on the MPEG 2 transition and scene encoder at every animation step. To provide a visual indication of the progress to the user the capture controller ensures that the canvas still paints the visible scene graph to the scene and adds a text overlay that indicates the percent of transitions executed.

A detailed example of an MPEG 2 transition and scene encoder according to an exemplary embodiment of the present invention is shown in . Raw scene data e.g. images text metadata etc. is delivered from the transition and screen capturer and provided to an object extraction unit a client rendered feature extraction unit and a video information extraction unit . The object extraction unit handling user interactable objects on the user interface screens and client rendered feature extraction unit handling e.g. hoverzoom and text features to be rendered by the client device operate under the control of the render location controller to extract information from the raw data stream and provide it to the ZSD encoder which encodes the extracted information using the scene description format described in detail below. None some or all of the ZSD encoded data can be sent within the MPEG data stream for example as part of the private data fields within MPEG frames using MPEG 2 data encapsulator while other ZSD encoded data can be transmitted using the OOB link described above with respect to .

The video information extraction unit operates to extract video information suitable for MPEG 2 encoding again under the control of the render location controller . The ability of render location controller to selectively determine which type of encoding to apply to particular data in this example MPEG or ZSD encoding and the benefits associated therewith are described in more detail below with respect to .

As used herein the term MPEG encoding is generic to MPEG 1 MPEG 2 and similar encodings although some exemplary embodiments of the present invention do specifically refer to MPEG 2 encoding. General details associated with MPEG encoding per se will be known to those skilled in the art and are further available in the form of draft standards e.g. ISO CD 11172 . An exemplary MPEG 2 encoder includes a plurality of unnumbered blocks which operate in accordance with the standard to perform MPEG 2 encoding an exception being motion estimation unit described in more detail below . One example of an MPEG encoder which provides a more detailed description of the unnumbered blocks of MPEG encoder can be found in the various MPEG 2 standards documents for example Test Model 5 documents which evolved as a joint effort between ITU T SG15.1 known then as CCITT SG XV Working Party XV 1 Experts Group on ATM Video Coding and ISO IEC JTC1 SC29 WG11 MPEG . Specifically the MPEG version of Test Model 5 is known as MPEG 93 225b and the ITU version of Test Model 5 is known as AVC 445b the disclosures of which are incorporated here by reference. MPEG encoded data is stored in the MPEG ZSD cache unit for subsequent transmission to the client device .

Of particular interest with respect to the exemplary MPEG 2 transition and scene encoder illustrated in is the encoder hint collector and motion estimator . One aspect of MPEG encoder in the MPEG 2 transition and scene encoder is its ability to quickly and efficiently provide a high level of compression of the MPEG data being encoded. Among other things this can be achieved by using knowledge of where each of the scenes are located relative to one another in the user interface which is defined a priori in exemplary user interfaces according to the present invention. This enables selective simplification of the standard MPEG motion estimation algorithm which in turn speeds up the MPEG encoding process and or reduces the amount of processing power that needs to be dedicated thereto. More specifically when encoding sequential MPEG frames in an MPEG data stream part of the information that is used to perform the encoding is information regarding where blocks of pixels have moved from one MPEG frame to the next MPEG frame and or backwards from a previous MPEG frame to a current MPEG frame . For example if a block of pixels in a first MPEG frame has simply moved to a new screen location in a second MPEG frame it is generally more efficient to determine and transmit a motion vector associated with that block of pixels than to re encode that entire block of pixels again and resend them. Similarly if that block of pixels has experienced a relatively uniform color difference e.g. by transiting through a lighting effect it is still efficient to provide a motion vector and some color difference information rather than retransmit the entire block of pixels.

In order to accommodate random object movement to support all types of e.g. video data compression standard MPEG motion estimation algorithms perform a search for blocks of pixel data determine which blocks of pixels have moved and in which direction from frame to frame. For example some searches call full pel searchs use 16 16 blocks while others called half pel searches use 16 8 blocks. These searches can become computationally expensive particularly for high definition video data and have been estimated to require up to 80 of the processing time power associated with the operations performed by a standard MPEG encoder e.g. without the modifications introduced by the encoder hint collector . Thus according to exemplary embodiments of the present invention motion estimation associated with MPEG encoding is simplified using the fact that the user interface being generated by these client server architectures does not involve random movement of objects. For example in transitioning between the exemplary user interface screens of and the image associated with Apollo 13 moves from a first position on a display screen to a second position on a display screen optionally with some magnification both positions being known a priori to the encoder hint collector which can calculate an MPEG motion vector therefrom.

Thus the encoder hint collector can pass the MPEG motion vector to motion estimation unit with a command to use the passed motion vector for performing MPEG compression rather than performing a search in accordance with standard MPEG techniques. However this use of knowledge of interrelated user interface screens to generate MPEG motion vectors may not always be able to generate a valid MPEG motion vector e.g. due to limitations on the number of bits assigned for expressing MPEG motion vectors . Accordingly encoder hint collector also has the capability to command motion estimation unit to employ the standard MPEG search algorithm to determine motion vectors on a frame by frame or other basis. In addition to either 1 using motion vectors which are generated entirely using the standard MPEG search algorithm or 2 using motion vectors which are generated entirely by the encoder hint generator without use of the standard MPEG search algorithm a third category of motion vectors which can be determined in accordance with the present invention are those which are calculated by the standard MPEG search algorithm having a search range which is limited in range based on the information available to the encoder hint collector .

Referring back again to MPEG data and scene description data generated by blocks and can be cached in memory device for retrieval as needed by the scene request processor . The scene request processor processes requests for scenes from client e.g. if the client user interface state machine receives an indication that the cursor associated with pointer has paused over the image associated with Apollo 13 then a request is sent back to scene request processor to initiate a hoverzoom scene described below or if the client user interface state machine receives an indication that the user wants to view a more detailed scene associated with Apollo 13 then a request is sent back to scene request processor to initiate that scene. The scene request processor returns MPEG 2 transitions and scene description data back to the client in response to the upstream requests. According to exemplary embodiments described in more detail below for certain upstream requests the scene request processor may dynamically determine whether MPEG data scene description data or some combination of both is appropriate to service the requests. A detailed example of the scene request processor is illustrated in .

Therein the client request processor coordinates all client interaction e.g. by interpreting client requests and dispatching those requests to the appropriate components within scene request processor . For example the client request processor tracks states and statistics on a per client basis and stores such information in database . An out of band OOB client communication component handles all communication with clients over OOB channels including responding to connection requests and extracting protocol requests. The video playback control function coordinates the operation of the MPEG 2 stream generation components e.g. the scene loop generator and the transition playback function . The scene loop generator component generates loops of the user interface scenes and transmits them when no transitions occur. The transition playback function loads MPEG 2 transition streams that were previously generated by the MPEG 2 transition and scene encoder e.g. via cache and streams them to the requested client. The transition playback function may serve multiple streams simultaneously. The MPEG 2 transport stream encapsulation unit updates the MPEG 2 transport stream as appropriate and forwards the stream to the UDP encapsulation unit which groups MPEG 2 transport stream packets together and sends them over UDP to a IP to QAM gateway not shown in the MPEG stream transmitter .

Referring again to MPEG stream transmitter on the server side and MPEG stream receiver and MPEG decoder on the client side enable the communication of both metadata e.g. data used to populate the text fields shown in the user interface screen of and content via a video streaming protocol link. The MPEG transmitter receiver and decoder can be implemented using off the shelf components and accordingly are not described in detail herein. However readers interested in more details relating to these elements as well as other exemplary interactive television system architectures in which the present invention can be implemented are referred to U.S. Pat. No. 6 804 708 to Jerding et al. the disclosure of which is incorporated here by reference. The on screen display OSD graphics controller receives data scene data from the client state machine and input from the cursor controller to generate overlay graphics and local animations e.g. zooming transitions for the user interface. The MPEG video data and the OSD video data output from decoder and OSD graphics controller respectively are combined by video combiner and forwarded to display device to generate the user interface. As mentioned above the DVD cover art images shown in are examples of user interface elements created using MPEG video data while the zoomed version of the Apollo 13 image in and the circular icons in the upper right hand corner of the user interface screen of are examples of user interface elements generated using scene description data.

Of particular interest for exemplary embodiments of the present invention is the client user interface state machine a more detailed example of which is provided in . The client user interface state machine interprets scene data and or scripts received from the scene request processor to present user interface scenes e.g. as shown in and on client devices . The client user interface state machine can also retrieve scene data and MPEG 2 transition clips from either the headend as represented by block or from a local hard disk drive . Those skilled in the art will appreciate that depending upon the system and or type of client device involved that only one data source may be present in a particular implementation of the present invention or that some other type of data source can be used. Out of band OOB communications can be used to provide signaling and commands to the client user interface state machine via an operating system OS e.g. PowerTV Linux Win32 etc. and operating system portal layer . The OS and OS porting layer can also track the user s activities with respect to the user interface and provide data to an event mapper function . Event mapper translates user interface data e.g. cursor movement voice command input motion of free space pointer etc. into events which may require some change in the user interface e.g. display change audio change zooming transition etc. For example when the user s cursor hovers over or passes over the image of Apollo 13 in the event mapper would receive raw cursor data from the OS and map that into for example a hoverzoom event which results in that image being slightly magnified as illustrated in and described in more detail below. As another example if the OS passed a button click through to the event mapper while the cursor was positioned over the magnified version of the Apollo 13 image in indicating that the user wanted more detail regarding this movie then the event mapper could identify a transition to detailed view event associated therewith leading to a transition to the user interface screen of .

Events detected by event mapper are queued in the event queue for processing by event processor . The event processor coordinates the activities of the client user interface state machine by receiving events from the event queue and dispatching them to the action library based on for example the currently active scene data and or script. The action library in conjunction with a scene data loader and various storage units operates to generate the change s to the currently displayed user interface screen based on the detected event as will be described in more detail below with respect to the discussion of scene data.

Having described some exemplary server client architecture for generating user interfaces according to exemplary embodiments of the present invention a second exemplary data format in addition to MPEG MPEG 2 which can be used in conjunction with this architecture will now be described. Although other data formats can be used in conjunction with the present invention this exemplary data format effectively creates a state machine that enables the client device to respond to user interactions and system events. This data format is arbitrarily extensible to support both very low powered client devices and high end client devices e.g. PCs. Other goals of this exemplary scene data format also referred to as ZSD include theme support future language support demo scripting and automated test support.

The ZSD format supports two types of scenes the exclusive scene and overlay scenes. Herein the exclusive scene is referred to simply as the scene since it occupies the full screen and contains the primary user interaction elements. Overlay scenes describe full or partial scenes that the client user interface state machine logically overlays on top of the exclusive scene. While the exclusive scene changes as the user navigates the overlay scenes may or may not change. This enables them to support features such as music controls global navigation bookmarks etc. that follow the user as they navigate from exclusive scene to scene. Exclusive scenes launch overlay scenes initially but overlay scenes may launch other overlays. Although it is possible to terminate all overlay scenes the overlay scenes control their own lifetime based on interaction from the user or based on the current exclusive scene.

The exclusive scene and all overlay scenes logically exist in their own namespaces. In order for ZSD elements to refer to elements in other scenes ZSD references as described herein could be modified to include a field to specify the namespace. Inter scene communication is useful for operations such as notifying overlay scenes what is in the exclusive scene. To support inter scene communication the sender triggers actions to generate events. These events are then dispatched by the event processor to each scene. When the event contains a Resource ID that ID is mapped to an equivalent resource in the destination scene. If the destination scene does not contain an equivalent resource the event processor moves on to test dispatching the event to the next scene.

Every exclusive scene passes through the following states sequentially on the client 1 Entered 2 Loaded 3 Steady State 4 Unloading and 5 Exited. When the exclusive scene s ZSD data is initially decoded the scene enters the Entered state. At this point the event processor fires the OnLoad event so that the exclusive scene can perform any initial actions. Once the event processor completes the OnLoad event dispatch process the exclusive scene enters the Loaded state. At this point the event processor may have pending events in its queue . The event processor clears out this queue and then transitions the exclusive scene to its Steady State. illustrates an exemplary exclusive scene life cycle using scene membership messaging to show event processing in all states. The process for unloading an exclusive scene is essentially the reverse of the load process. For this case a GoToScene or other scene changing action initiates the unload process. At this point the exclusive scene changes to the Unloading state. Once all ZSD unload processing completes the process transitions to the Exited state wherein the client may optionally retain some or all of the exclusive scene s ZSD data. The changes in the exclusive scene s state are communicated to all currently loaded overlay scenes so the overlay scene can take action if needed .

Overlay scenes exist independent and on top of the exclusive scene. For example in the three icons depicted in the upper righthand corner home up arrow and TV can be implemented as overlay scenes on the exclusive scene the images of various DVD covers implemented in the MPEG layer . Another example not shown in is the provision of volume control and or channel selection user interface objects as overlay scenes. Termination of an overlay scene can be accomplished from within the scene itself or by request from the exclusive scene. Additionally SceneMembershipNotifcation events can be used to limit the lifetime of an overlay scene to a particular set of exclusive scenes as shown for example in . Each of the exclusive scenes that belong to this scene group would send a SceneMembershipNotification message when they are loaded. The overlay scene associated with this scene group would use the ExclusiveSceneChange events and the SceneMembershipNotification message to tell if the overlay scene should stay loaded or should terminate itself. As long as it receives a SceneMembershipNotifaction that matches its Scene Group the overlay screen can stay loaded. Triple tables mentioned in are described in more detail below.

According to one exemplary embodiment of the present invention each scene contains the following descriptive information 

An exemplary scene data format according to the present invention has four fundamental data types sometimes referred to herein as elements specifically objects events actions and resources. At a high level objects describe scene components such as the bounds for buttons and icons in the MPEG layer overlay text and overlay images. Events describe the notifications that are pertinent to the scene. These include mouse pointer move events keyboard events application state change events etc. Actions describe responses to events such as going to another scene and finally resources contain the raw data used by objects events and actions e.g. image data. Each of these data types are described in more detail below.

Exemplary object types and parameters associated therewith including an optional set of properties according to an exemplary embodiment of the present invention are described in tables 5 8.

Like the other scene description format elements each event is assigned a globally unique value. Some event types employ filters to constrain the actions that they would trigger. For example the OnKeyPress event uses the key of interest. In addition to filters events can push resources onto the action stack described below. Actions may use the information on the stack to modify their behavior.

Exemplary event types are listed in Table 9 below. Overlay scenes affect the propagation of events by the dispatcher. Dispatch semantics are abbreviated in the table as follows 

In operation of the architectures and methods described herein the result of an event on an object is an action. Actions may be linked together in a ZSD Action Table to form programs. To facilitate parameter passing to actions from events and to linked actions a ZSD interpreter maintains an action stack. The action stack is initialized before dispatching the first action in an action list with the following items in order 

Exemplary resources which can be used in conjunction with the present invention are listed below in Table 11.

According to an exemplary embodiment of the present invention the scene description format groups all scene interaction information into five tables the object table the event table the action table the resource table and one or more triple tables as described below in Tables 12 17. This division into tables eliminates most redundant information and enables quick lookup of interaction behavior on low end clients .

Client devices without local storage request scenes and transitions from the server . An exemplary set of messages which can be used to perform this function is provided below in Table 18. The client server link can for example be made over an Ethernet connection QPSK channels used by cable networks currently for OOB communications or any other protocol or type of connection. Those skilled in the art will appreciate that this message set is purely exemplary and that messages can be added or deleted therefrom.

As mentioned above one feature of exemplary client server architectures and methods according to the present invention is to provide the capability for sophisticated user interfaces to be generated at the client side while taking into account the relatively small amount of available memory and or processing power associated with some existing client devices. One example of the ways in which the above described systems and methods address this issue can be seen with respect to the user interface interaction referred to herein as a hoverzoom e.g. the process whereby when a user rolls a cursor over and or pauses an indicator relative to a media item that can be selected the image associated therewith is magnified so that the user can easily see which object is poised for selection an example of which is illustrated in and .

There are a number of challenges associated with implementing a hoverzoom feature in bandwidth limited systems such as interactive television systems wherein the client devices have limited memory and or processing power. Consider the example wherein the user interface screen illustrated in is rendered using MPEG data streams transmitted from the user interface server to the client containing the cover art images associated with various movies. This visual portion of the user interface screen will be referred to herein as the background layer. When the event mapper and event processor recognize that the user has triggered a hoverzoom response a foreground layer e.g. the magnified version of the Apollo 13 image is generated and used to modify the user interface screen of . There are several possibilities for providing the data used to transition from the user interface screen shown in to the user interface screen shown in . One way to implement the hoverzoom effect is to have the user interface server transmit complete sets of MPEG data corresponding to both the background layer and the foreground layer to the client . However when one considers that the user can roll the cursor over a potentially very large number of screen objects in the user interface e.g. dozens or hundreds quite rapidly the amount of data needed to be transmitted by the user interface server could be quite large to implement this exemplary embodiment of the present invention resulting in additional delay in rendering the screen transitions on the client device .

Moreover it can be seen from comparing with that a significant portion of the pixel data associated with the unzoomed version of is reused in creating the hoverzoomed version of . Thus according to another exemplary embodiment of the present invention the relationship between pixels in the background layer and the foreground layer can be determined and used to reduce the amount of data that needs to be transmitted to the client device to generate a hoverzoom effect. Depending upon the object to be magnified as part of the hoverzoom effect this relationship can be relatively simple or somewhat more complex. For example enlarging the size of the rectangular DVD cover art images of primarily involves enlarging a rectangular image to occlude neighboring images as part of the transition. On the other hand more complex shapes e.g. a doughnut shaped object with a hole in the center present more complex situations for generating a hoverzoom effect. Consider that as the doughnut shaped object is enlarged the hole in the middle will expand such that background layer pixels that were previously hidden become revealed after the hoverzoom effect has occurred.

According to one exemplary embodiment of the present invention each pixel in the foregoround version of the image is categorized as being one of 1 completely opaque can extract pixel color from background layer so do not need to resend for foreground layer generation 2 transparent irrelevant so do not need to resend for foreground layer 3 translucent e.g. pixels around edges of image can have anti aliasing applied thereto need to send foreground layer data for these pixels and 4 null e.g. doughnut hole pixels which reveal background pixels need to send background layer pixels since those cannot necessarily be extracted from background layer that was originally sent to create the unzoomed interface screen . This categorization can be done a priori using any desired technique including manual observation and or using the pseudocode processing techniques described below and a foreground background map is generated wherein each pixel in the foreground layer is categorized. A hoverzoom map can be stored for each image for which a hoverzoom effect can be triggered in the user interface.

for node scenegraph.rootO node foreground node node next node if node bounds within foreground bounds 

Hoverzoom processing in accordance with this exemplary embodiment of the presents invention is generally illustrated in . Therein an MPEG background version of the image and an unzoomed version of the image to be magnified for example Apollo 13 in e.g. PNG or JPEG are provided. The background image is combined with the unzoomed version of the image and transmitted to the client device in the MPEG data stream after compression at step . The foreground background map described above is retrieved from storage at step and used to determine which pixel data associated with the foreground layer and the background layer needs to be transmitted. That data is encoded compressed at steps and saved as a ZSD image file and transmitted to the client device . Although this exemplary embodiment of the present invention transmits this information as scene data ZSD data outside of the MPEG data stream it can alternatively be embedded in the MPEG data stream.

As will be appreciated by reading the foregoing discussion of hoverzoom techniques in accordance with an exemplary embodiment of the present invention some of the challenges associated with generating sophisticated user interfaces e.g. which employ zooming at client devices connected to for example a cable network can be addressed by intelligent selection of an encoding stream for particular data to be transmitted. In the foregoing hoverzoom example background data was sent using the MPEG encoding stream available in such networks while the foreground information was sent using a different type of encoding described above handled for presentation through the OSD layer. However exemplary embodiments of the present invention contemplate that other server client data transfers may benefit from selectively deciding at one of the upstream nodes which is supplying data to the client device which type of encoding data stream is appropriate for data to be transmitted in particular for data associated with zooming user interfaces.

This general concept is illustrated in . Therein data is evaluated at block to determine whether it is first data or second data and selectively determining a type of encoding and associated transmit data stream for handling that data. First and second data can be different types of data or the same type of data having different characteristics. An example of the foregoing is the hoverzoom data background data being first data and foreground data being second data . An example of the latter is text. MPEG encoding is not particularly efficient for encoding text and accordingly it may be desirable to encode text under certain circumstances using another type of encoding e.g. if the text to be transmitted is less than a predetermined font size e.g. 16 point .

In some cases such client devices will continue to have difficulties rendering screens associated with zoomable user interfaces ZUIs as well as other applications such as Internet browsing. For example embedded platforms which typically run on such thin client devices e.g. set top boxes and the like have access to limited memory processing power and therefore cannot handle certain content and application support. For example it would be desirable to provide a full featured Internet browsing capability in addition to or as an alternative to the afore described ZUIs on a user s television s e.g. in the living room. Another challenge which arises with such client devices is their lack of support for certain types of media and associated codecs. For example frequent updates and versions are typically made available to Flash codecs on an ongoing basis. However embedded platforms which operate on thin client devices may only have access to out of date codecs in some cases several versions out of date due to OEM practices associated with the provision of such software. Accordingly it becomes difficult or impossible to render certain types of content on the television through such thin client devices.

According to exemplary embodiments this challenge is addressed by adding a personal computer PC to the processing chain in e.g. the afore described systems. As generally shown in a home personal computer can be inserted into the processing stream between the client device and the headend system to assist the thin client device in rendering content provided from the headend . The content can be rendered in accordance with one or more software applications SAs running on the home computer . According to one exemplary embodiment software application can be a zoomable user interface which provides access to media as described above. According to another exemplary embodiment software application can be an Internet browser described below in more detail with respect to . According to still another exemplary embodiment software application can be both an Internet browser and a ZUI and or other applications e.g. office applications media applications phone and communications applications drawing applications etc.

In such a combination more of the processing can be performed by the relatively local home personal computer which will typically have more memory and or more processing bandwidth than the thin client device . Consider the example shown in wherein the home personal computer operates an Internet browser acting as a software application which is remoted to the TV as follows. Suppose that a user e.g. pointing toward the TV using a 3D pointing device and providing keystroke inputs via a virtual keyboard displayed on the TV requests access to a particular web page . This user input information is relayed to user input function in the client device which passes the information on to a corresponding function of the home PC . Home PC uses the e.g. input address and the browser application to access web page . Alternatively other types of devices e.g. gaming consoles network attached storage NAS devices cell phones PDAs etc. which have enough processing capability as well as access to the desired interface and other support features could be used in place of home PC .

The web page typically has one or more objects also sometimes referred to as rectangles associated therewith. In this purely illustrative example web page has a video rectangle and an audio rectangle associated therewith. The PC s processor not shown scans the web page and more precisely the HTML code associated therewith to identify how many and what type of rectangles are present on the web page . The PC then matches the identified information with the known capabilities of the client to determine what type of subsequent processing if any is needed before it sends information about the web page over to the client device for display on TV . For example suppose that the client device supports MPEG encoded video i.e. has an MPEG codec but does not support Flash encoded video content.

If the home PC scans a web page and determines that the web page has a Flash encoded rectangle it will first re encode block that particular rectangle to MPEG so that the thin client can fully display the web page on the television . Once selected video rectangles are re encoded at block they are passed through to the client device via video transmit function which may perform other coding operations associated with transmission of the video data to video replay function which e.g. decodes the received video data for handling by the client s graphics chip . Similarly static graphics and audio rectangles associated with the web page can be identified as part of the HTML scanning process and coded directly for transmission from the home PC via screen transmit and audio transmit functions respectively. The resulting data streams from blocks and are received by corresponding functions and on the client side and used to recreate the web page on the television .

As described above according to exemplary embodiments the home PC has the capability to re encode video content into a format useable by the client device . Such a transcoding operation may for example be performed at either the signal level or the rendering level of the processing. According to alternative exemplary embodiments the home PC is able to transmit new codecs as well as codec updates to the client device for its use. Initially the home PC and client device communicate such that home PC understands which codecs the client device has. When a request comes from the client device which results in a video media that the client device does not support the home PC can either translate the video into a format known by the client device or transmit the new codec to the client device for its use followed by the desired video content.

A plurality of re encoding functions can be provided as video plug ins for home PC to adapt various content which may be found on web pages to the known capabilities of the client device which capabilities such as the types and or versions of video codecs provided in the client can be stored by the home computer e.g. in a memory associated therewith. According to one exemplary embodiment although the type of application or applications running on the home PC may vary the interface and via which it provides data to the client can be the same i.e. a standardized interface for remoting a home PC to the television via a client device such as a wireless home network e.g. a LAN.

Systems and methods for processing data according to exemplary embodiments of the present invention can be performed by processors executing sequences of instructions contained in a memory device not shown . Such instructions may be read into the memory device from other computer readable mediums such as secondary data storage device s . Execution of the sequences of instructions contained in the memory device causes the processor to operate for example as described above. In alternative embodiments hard wire circuitry may be used in place of or in combination with software instructions to implement the present invention.

The exemplary embodiments described above provide methods and systems for augmenting the capabilities of a client device e.g. a thin client device such as a set top box with a personal computer . Communications node can contain a processor or multiple processor cores memory one or more secondary storage devices software application SA and a communications interface . Processor is capable of processing instructions e.g. software instructions in support of a client device to increase the client devices capabilities. For example processor can receive media desired by the client device and translate it into a format usable by the client device prior to transmitting the translated media. As such communications node is capable of performing the tasks of a home PC or other device as described in the exemplary embodiments herein to augment the capabilities of a client device .

Utilizing the above described exemplary systems according to exemplary embodiments a method for augmenting a client server is shown in the flowchart of . Initially a method for augmenting a client device includes the steps of receiving a request to perform at least one function in step processing the request to perform the at least one function in step performing the at least one function which results in a first output in step selectively translating the first output into a format usable by the client device into a second output in step and transmitting either the first output or the second output to the client device in step .

The above described exemplary embodiments are intended to be illustrative in all respects rather than restrictive of the present invention. Thus the present invention is capable of many variations in detailed implementation that can be derived from the description contained herein by a person skilled in the art. For example although MPEG encoding and MPEG data streams have been described in the foregoing exemplary embodiments it will be appreciated that different types of encodings and data streams can be substituted thereof in part or in whole e.g. video encodings used in Windows Media based content and the like. Moreover although MPEG image and or video data is described as being transmitted through all or part of a cable network the present invention is equally applicable to systems wherein the image and or video data is available locally e.g. on a home disk or from a local server. All such variations and modifications are considered to be within the scope and spirit of the present invention as defined by the following claims. No element act or instruction used in the description of the present application should be construed as critical or essential to the invention unless explicitly described as such. Also as used herein the article a is intended to include one or more items.

