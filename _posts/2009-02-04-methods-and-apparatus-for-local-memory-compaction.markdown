---

title: Methods and apparatus for local memory compaction
abstract: Methods, apparatus and computer software product for local memory compaction are provided. In an exemplary embodiment, a processor in connection with a memory compaction module identifies inefficiencies in array references contained within in received source code, allocates a local array and maps the data from the inefficient array reference to the local array in a manner which improves the memory size requirements for storing and accessing the data. In another embodiment, a computer software product implementing a local memory compaction module is provided. In a further embodiment a computing apparatus is provided. The computing apparatus is configured to improve the efficiency of data storage in array references. This Abstract is provided for the sole purpose of complying with the Abstract requirement rules. This Abstract is submitted with the explicit understanding that it will not be used to interpret or to limit the scope or the meaning of the claims.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08661422&OS=08661422&RS=08661422
owner: Reservoir Labs, Inc.
number: 08661422
owner_city: New York
owner_country: US
publication_date: 20090204
---
This application is related to and claims the benefit of priority to U.S. Provisional Application Ser. No. 61 065 294 entitled SYSTEM APPARATUS AND METHODS FOR SOURCE CODE COMPILATION filed Feb. 8 2008 the entirety of which is hereby incorporated by reference.

This invention was made with Government support under contract no. F30602 03 C 0033 awarded by Defense Advanced Research Projects Agency W31P4Q 07 0147 awarded by Defense Advanced Research Projects Agency FA8650 07 M 8129 awarded by the Office of the Secretary of Defense and W9113M 07 C 0072 awarded by the Missile Defense Agency. The Government has certain rights in the invention.

The present invention generally concerns computer programming. More particularly the invention concerns a system methods and apparatus for source code compilation.

The progression of the computer industry in recent years has illustrated the need for more complex processor architectures capable of processing large volumes of data and executing increasingly complex software. A number of systems resort to multiple processing cores on a single processor. Other systems include multiple processors in a single computing device. Additionally many of these systems utilize multiple threads per processing core. One limitation that these architectures experience is that the current commercially available compilers can not efficiently take advantage of the increase of computational resources.

In the software design and implementation process compilers are responsible for translating the abstract operational semantics of the source program into a form that makes efficient use of a highly complex heterogeneous machine. Multiple architectural phenomena occur and interact simultaneously this requires the optimizer to combine multiple program transformations. For instance there is often a trade off between exploiting parallelism and exploiting locality to reduce the memory wall i.e. the ever widening disparity between memory bandwidth and the frequency of processors. Indeed the speed and bandwidth of the memory subsystems are a performance bottleneck for the vast majority of computers including single core computers. Since traditional program optimization problems are associated with huge and unstructured search spaces this combinational task is poorly achieved by current compilers resulting in poor scalability of the compilation process and disappointing sustained performance of the supposedly optimized program.

Even when programming models are explicitly parallel threads data parallelism vectors they usually rely on advanced compiler technology to relieve the programmer from scheduling and mapping the application to computational cores and from understanding the memory model and communication details. Even provided with enough static information and code annotations OpenMP directives pointer aliasing separate compilation assumptions traditional compilers have a hard time exploring the huge and unstructured search space associated with the mapping and optimization challenges. Indeed the task of the compiler can hardly be called optimization anymore in the traditional meaning of reducing the performance penalty entailed by the level of abstraction of a higher level language. Together with the run time system whether implemented in software or hardware the compiler is responsible for most of the combinatorial code generation decisions to map the simplified and ideal operational semantics of the source program to a highly complex and heterogeneous target machine.

Generating efficient code for deep parallelism and deep memory hierarchies with complex and dynamic hardware components is a difficult task. The compiler along with the run time system now has to take the burden of much smarter tasks that only expert programmers would be able to carry. In order to exploit parallelism the first necessary step is to compute a representation which models the producer consumer relationships of a program as closely as possible. The power of an automatic optimizer or parallelizer greatly depends on its capacity to decide whether two portions of the program execution may be run one after another on the same processing element or on different processing elements or at the same time in parallel . Such knowledge is related to the task of dependence analysis which aims at precisely disambiguating memory references. One issue is to statically form a compact description of the dynamic properties of a program. This process is generally undecidable and approximations have to be made.

Once dependence analysis has been computed a compiler performs program transformations to the code with respect to different sometimes conflicting performance criteria. Any program transformation must ultimately respect the dependence relations in order to guarantee the correct execution of the program. A class of transformations targeting the loop nests of a program such as DO loops in the FORTRAN language and for and while loops in languages derived from the C language are known to account for the most compute intensive parts of many programs.

Traditional optimizing compilers perform syntactic transformations transformations based on a representation that reflects the way the program source code text was written such as the Abstract Syntax Tree making the optimizations brittle since they are highly dependent on the way that the input program is written as opposed to the more abstract representation of the program s execution offered by the polyhedral model. Moreover syntactic transformations are not amenable to global optimizations since the problem of optimally ordering elementary syntactic transformations is yet unsolved. Many interesting optimizations are also not available such as fusion of loops with different bounds or imperfectly nested loop tiling.

In some situations such as in high performance signal and image processing the applications may primarily operate on dense matrices and arrays. This class of applications primarily consists of do loops with loop bounds which are affine functions of outer indices and parameters and array indexing functions which are affine functions of loop indices and parameters. Other classes of programs can be approximated to that class of programs.

One significant area of concern in these large scale systems is memory management. For example in a program a large multi dimensional array may be allocated and used to store data. This large block of data is typically stored in memory in contiguous memory cells. Certain operations on the array may not access all portions of the data. For example in nested loops an outer loop may be indexed by the column of the array and an inner loop may be indexed by the rows of the array. In a situation where the loop operation only accesses a portion of the elements of the array it would be inefficient to transfer the entire array to a processing element that is assigned the access task. Further since portions of the array are not accessed the loop indices may be rewritten for local access on a processing element.

There have been a number of approaches used to implement these program transformations. Typical goals of these approaches include reducing the memory size requirements to increase the amount of useful data in local memory and to reduce communication volumes. One such algorithm is described in U.S. Pat. No. 6 952 821 issued to Schreiber. Schreiber s method is applicable to non parametric rectangular iteration spaces and employs the Lenstra Lenstra Lovasz LLL lattice basis reduction algorithm. Schreiber s methods are additionally incapable of addressing data with non convex sets of accessed data.

Therefore a need exists for more efficient compiler architectures that optimize the compilation of source code.

The present invention provides a system apparatus and methods for overcoming some of the difficulties presented above. Various embodiments of the present invention provide a method apparatus and computer software product for a class of automatic program transformations that reduce the memory size requirements by relocating and compressing the memory accesses of a program that includes loop nests with arbitrary affine indices. Exemplary methods extract a lattice of points within the iteration domain to handle iteration domains with strides for which the values of loop counters differ by more than a unit for loop iterations executed consecutively. Other provided methods operate on programs that contain arbitrary affine array index functions and in some instances where the program transformation handles arbitrarily complex data footprints.

An exemplary method includes receiving program source code containing loop nests with arbitrary parametric affine iteration domain containing at least one array. The method identifies inefficiencies in memory usage where the inefficiencies are related to access and the memory footprint of the arrays. The method further allocates at least one local array and maps a portion of the received arrays to one or more of the local arrays. The mapping reduces the memory size requirements and the memory footprint of the arrays.

A further embodiment provides a local memory compaction module that assists a processor in the optimization of source code. Other embodiments provide computing apparatus and computer software products that implement the described methods.

It will be recognized that some or all of the figures are schematic representations for purposes of illustration and do not necessarily depict the actual relative sizes or locations of the elements shown. The Figures are provided for the purpose of illustrating one or more embodiments of the invention with the explicit understanding that they will not be used to limit the scope or the meaning of the claims.

In the following paragraphs the present invention will be described in detail by way of example with reference to the attached drawings. While this invention is capable of embodiment in many different forms there is shown in the drawings and will herein be described in detail specific embodiments with the understanding that the present disclosure is to be considered as an example of the principles of the invention and not intended to limit the invention to the specific embodiments shown and described. That is throughout this description the embodiments and examples shown should be considered as exemplars rather than as limitations on the present invention. Descriptions of well known components methods and or processing techniques are omitted so as to not unnecessarily obscure the invention. As used herein the present invention refers to any one of the embodiments of the invention described herein and any equivalents. Furthermore reference to various feature s of the present invention throughout this document does not mean that all claimed embodiments or methods must include the referenced feature s .

The trend of increasing the frequency at which processors perform computations seems to have come to an end. Power consumption and control complexity have reached such high levels that manufacturers are backing out of this design path. Current machines have evolved to multiprocessor architectures on a chip with increasingly many cores per chip and multiple threads per core. This trend is expected to dramatically increase reaching thousands of cores per chip in the next few years. Thus modern computers increasingly need to exploit parallelism at different levels to provide sustained performance. On the other hand parallel programming techniques have not evolved at the same speed and the gap between theoretical machine speed and actual utilization continues to increase.

Compilers are responsible for translating the abstract operational semantics of the source program i.e. a text description of what the program s execution is supposed to perform into an executable form that makes efficient use of a highly complex heterogeneous machine. Multiple architectural phenomena occur and interact simultaneously within the targeted computer during the execution of the program this requires the optimizing compiler to combine multiple program transformations in order to define a program execution that takes advantage of those architectural phenomena. For instance when targeting computers that have multiple processing elements multi core computers there is often a trade off between exploiting more processing elements simultaneously parallelism and exploiting data access locality to reduce memory traffic. Indeed the speed and bandwidth of the memory subsystems are almost always a bottleneck. The problem is typically worse for multi core computers. Since in traditional compilers optimization problems are associated with huge and unstructured search spaces this combinational task is poorly achieved in general resulting in poor scalability and disappointing sustained performance of the supposedly optimized program.

Generating efficient code for deep parallelism and deep memory hierarchies with complex and dynamic hardware components is a difficult task the compiler and run time system has to take the burden of tasks that only expert programmers would be able to carry. In order to exploit parallelism the first necessary step is to compute a representation which models the producer consumer relationships of a program as closely as possible. The power of an automatic optimizer or parallelizer greatly depends on its capacity to decide whether two portions of the program execution may be interchanged or run in parallel. Such knowledge is related to the task of dependence analysis which aims at precisely disambiguating memory references. The issue is to statically form a compact description of the dynamic properties of a program. Forming a precise description is generally undecidable and approximations have to be made.

Once dependence analysis has been computed a compiler performs program transformations to the code with respect to different sometimes conflicting performance criteria. Any program transformation must ultimately respect the dependence relations in order to guarantee the correct execution of the program. A class of transformations targeting the loop nests of a program such as DO loops in the FORTRAN language and for and while loops in languages derived from the C language are known to account for the most compute intensive parts of many programs. The polyhedral model is a representation of a program s structure particularly suited for expressing complex sequences of loop nests complex sequences of loop nest transformations and other relevant information such as for instance dependences communications and array layouts.

A polyhedron is defined as a set of points verifying a set of affine inequalities and equalities on a number of variables. There exist alternate but equivalent definitions for polyhedrons such as the one based on a combination of vertices rays and lines proposed by Minkowski. There are also alternate representations often based on the alternate definitions. While the present disclosure teaches using one of those definitions and representations to illustrate the various embodiments various embodiments are in no way restricted to a particular definition or representation.

A polyhedral domain is defined as a finite union of polyhedrons. One of the main interests in using polyhedral domains is that they provide a precise representation of sets and relations among sets on which many optimization problems can be phrased and solved using a rich set of algorithms which are mostly available in the literature. Some embodiments of the sets in question represent loop iterations mono and multi dimensional data sets sets of processing elements data transfers synchronizations and dependences. Thus essential characteristics of the execution of a program can be summarized into compact mathematical objects polyhedrons which can be manipulated and transcribed into an executable program that has desired execution properties.

By considering a subset of the variables of a polyhedron as symbolic constants also called parameters it is possible to perform program optimizations and parallelization as a function of the symbolic constants. Hence programs involving loops that depend on a constant value that is not known at the time when compilation is performed but only when the program is executed can be modeled using polyhedrons that are defined as a function of those constant values. A polyhedron involving parameters is called a parametric polyhedron. Similarly a parametric polyhedral domain is defined by a finite union of parametric polyhedrons. For instance the set of values that the counters of a loop nest reach during the execution of the loop nest is represented by the loop nest s iteration domain . The iteration domain of the following loop nest using the C language s syntax where F is a C function call can be written as the parametric domain P n i j Z 5 i n 0 j 10 j i 

While most of the transformations applied to the polyhedral representation of a program are defined for any element of the polyhedral domain to transform a class of more complex and precise transformations is obtained by partitioning the vector space in which the polyhedral domain is defined into sub polyhedrons and by defining a different transformation for each polyhedron of the partition. The resulting transformation is called a piecewise transformation. For example consider the transformation that takes two numbers i and j and computes three numbers x y and z as x 2i 1 y i j 2 z 3j 4 when i is greater than j and x i y i j 3 z 2j when i is less than or equal to j. It is a piecewise affine function since it has different definitions for each set of values i j and i j which define a partition of the i j vector space.

The context of various embodiments the use of polyhedral representations to perform complex optimizations on programs either independently or within a system of optimizing components. An exemplary embodiment of such a system is illustrated in where it is described as being part of a compiler. Flow of the exemplary embodiment starts in block where the compiler is processing a program. Flow continues in block where the compiler analyzes the program to decide if there are portions of the program that should be optimized and mapped using a polyhedral system. If it is the case the flow goes to block where the compiler provides the system of optimizing components with a polyhedral representation of a portion of the program to be optimized. If not the compiler continues to process the program without using the system of optimizing components and completes. The components of the system are in charge of a part of the global optimization of the input program. In the flow of the embodiment illustrated in the polyhedral representation of the input code is analyzed in block to produce dependence information. Flow continues in block where such information is used in a local memory compaction component or module that modifies array layouts in a way that removes some dependencies schedules loop iterations in a way that exposes loops that scan independent iterations and schedules the execution of operations using the same data to be executed within a close time interval. Flow continues in block where the modified polyhedral representation of the program is processed by another optimizing component which partitions the represented loop operations into entities called tasks which have good data locality properties they access a data set that involves an optimized use of the memory subsystem of the target computer and assigns a processing element of the target machine to each task. In this exemplary embodiment the flow continues to decision block which decides which block is next in the flow as a function of the target machine. If the target machine requires the execution of explicit communication commands to transfer data to and from its processing elements flow goes to block where the representation of the program thus modified is then processed by a series of optimizing modules which define a new layout for data that is transferred to a processing element s local memory. Otherwise the flow goes to block . From block flow continues to block where a representation of the explicit communications is produced based on polyhedrons and then to block where the execution of the communications are scheduled for parallel execution with the tasks using multi buffering. Whether the target machine requires explicit communications or not the flow continues to block where an optimizing component processes the polyhedral representation of the program obtained from the previous components by inserting a polyhedral representation of synchronization operations which ensure that the execution of the modified program produces the same results or similar results as the original input program. The flow of the exemplary embodiment then goes to block where an optimizing component partitions the tasks into subtasks whose execution reduces traffic between the processing elements memories and their registers. Then in block a polyhedral representation of commands that trigger the execution of a series of tasks on the different processing elements of the target machine and that wait for the completion of those is generated by the next optimizing component. Finally in block the polyhedral representation of the optimized program is transformed by polyhedral code generation component into a representation Abstract Syntax Tree high level language code or a compiler s internal representation that can be either processed by a compiler or processed further by the user. In the exemplary embodiment the flow continues back to block where it may cycle again through the whole flow if there is more code to be optimized using the system of optimizing components.

In contrast to compilers based on polyhedral domains traditional loop oriented optimizing compilers typically perform syntactic transformations. As a result many interesting optimizations are often not available such as fusion of loops with different bounds or imperfectly nested loop tiling.

In some embodiments the optimizing components or modules comprise processor executable code that when executed by a processor convert source code into other forms of source code or in some instances machine code. In other embodiments various modules may be implemented in hardware such as monolithic circuits Application Specific Integrated Circuits ASIC or Field Programmable Gate Arrays FPGA . These modules may comprise software hardware firmware or a combination of these implementations. It is important to note that various embodiments are illustrated in specific programming languages these illustrations are mere examples and the scope is not therefore limited to any particular programming language.

Embodiments of a provided optimization module described above as local memory compaction are illustrated in . illustrates the flow of a provided method for local memory compaction. Flow begins in block where source code is received into memory. In this embodiment the source code represents loops with arbitrary parametric affine iteration domain and contains at least one array reference. An array reference is an operation that represents an access typically a read or a write to an array. Such a reference represents either explicitly or implicitly for instance by using programming language conventions a function to retrieve the memory address of an element of the array. In loop programs that function is typically a direct or indirect function of the loop indices and of some loop constant values. For instance in C arrays are typically referenced through mono and multi dimensional affine functions of some input values. In the C language the declaration of an array includes parameters called array size which implicitly define the address of an array element as a function of the input values to references to this array. Declaring char A 100 200 allocates an array of 20000 elements 100 200 named A and defines that for any two integer values x and y the memory address of the element of A referenced through A x y is b 200x y where b is a value called the base address of array A. b is constant for each array and is determined at some point in the compilation process. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. In one embodiment the inefficiencies are related to access and memory footprint of the array. Flow then continues to block where at least one local array is allocated and in block a portion of the array with inefficient memory usage is mapped into the local array. The mapping portion of the module outputs code that is more efficient than the original code in terms of the memory size requirements of the local array versus the original array. In some embodiments the accessed data is arbitrarily complex. In further embodiments the mapping produces a piecewise affine index function for the local arrays. Other embodiments include the rendering of a visualization of the optimized code on a monitor.

Arrays are typically allocated sets of contiguous memory blocks. Some loop operations may access only portions of the allocated memory. When reorganizing the data layout for a specific processor there is an opportunity to take advantage of the inefficiencies in memory access requirements versus the actual utilization of the array. For example given the following code fragment 900 000 contiguous memory blocks are allocated but only 100 are accessed in this operation. Furthermore access to the array is not contiguous but contains gaps and thus will have less than optimal locality. Thus keeping the original data layout and array size in a remote processor is extremely inefficient. Moreover if there are less than 900 000 blocks available in the local memory the local memory cannot hold the entirety of the array and the program cannot be executed properly. In the provided code fragments we are using . . . to elude other operations which do not have any specific illustrative purpose.

One embodiment of a provided method illustrated in would map this code fragment into a local array with 100 elements. An exemplary mapping would produce the following pseudo code fragment in which the storage requirement of a local array is reduced from 300 300 elements to the optimal 100 elements.

One feature of this embodiment is that it provides a method of compacting local memory in a computing apparatus. This method provides a more efficient memory structure in terms of both access to the elements and the amount of memory occupied by the data that is actually accessed. The memory requirements are reduced from the initial allocation to an allocation that is large enough to contain the data that is actually used in the operations. In contrast to other methods the provided method handles loops whose iteration domains are non rectangular and loops that have a parametric iteration domain. In this document we refer to polyhedral iteration domains that are either non rectangular or parametric or both as arbitrary parametric iteration domains . In addition the provided methods handle non convex accessed data sets. The provided embodiments are very useful in image and video processing. Imaging applications typically utilize significant multi dimensional arrays where data representations of physical objects and systems are stored. Many image processing steps such as discrete wavelet transforms for example only utilize discrete portions of the stored data. In these situations various embodiments provide significant optimizations to local data storage.

Another embodiment of a provided method is illustrated in . In this embodiment flow begins in block where source code is received in memory. Similar to the above embodiment the source code contains loops with arbitrary parametric iteration domain and contains at least one array reference. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. Flow then continues to block where at least one local array is allocated and in block a portion of the array with inefficient memory usage is mapped into the local array. In this embodiment mapping block includes partitioning references to form compatible references in block determining a relation within compatible references in block grouping compatible references based on the relation in block performing algebraic simplification in block and performing geometric arrangement through re indexing the elements of the local array in block . In some embodiments the set of references partitioned are references that access a portion of the array. The following pseudo code example illustrates this embodiment.

In this case all three references to array A are disjoint in that they access disjoint portions of the array. In this case they are transformed into three local arrays A2 A3 and A4 in the following manner.

Performing transformations of the way data are allocated in memory i.e. transforming the data layouts has a combinational aspect since the data sets accessed through each array reference may overlap with one or more data sets accessed by other array references. Since each one of those overlaps entail constraints in the way that data layouts can be transformed analyzing all the combinations of overlaps for all the references is a source of high computational complexity. Hence references are grouped into sets in such a way that data accessed through one set of references does not overlap data accessed through another set of references. In this embodiment references of the same set are called compatible references . Since there is no overlap among sets of compatible references the following parts of the memory layout transformation which consider the overlaps can be applied independently to each set of compatible references. In particular they will decide if the overlapping data sets accessed by a set of compatible references should be partitioned further and how.

In some embodiments compatible references are identified by overlapping memory footprints during the execution of a particular subset of loop iterations. In an exemplary embodiment the provided method identifies array references having overlapping memory footprints duplicates a portion of the identified references and associates each of the duplicates with disjoint subsets of the memory footprint. An example pseudo code illustrates this embodiment.

The two references A i j and A j i overlap when i j. However if the references are allocated together it is impossible to reduce the local memory usage using only affine transformations. This is because the data footprint of the two references is a 2 dimensional set a cross while the data footprints of the individual references are both 1 dimensional. In order to compute better allocations in situations like this one embodiment first estimates how much overlapping is in the references. If the references are read only and if the overlapping data set is a small percentage of the overall data set the embodiment splits the references into two distinct references to one dimensional data sets. In the above example the embodiment will generate the following local memory allocation. Note that the center element of the data foot print A i i has been replicated and put into the locations A I i and A2 i .

The geometric re arrangements provided by a further exemplary embodiment are defined by a piecewise affine transformation. In other words the transformation applied to the references is defined as a set of functions each element of the set being valid within a polyhedral domain of the loop values the parameters and the coordinates of the data accessed through the set of compatible references. In an exemplary embodiment when some of the data accessed by a set of compatible references are written by some of the references the written data subset and a subset of the data set that is only read define a partition for the piecewise affine transformation. Consider the program represented by the following pseudo code 

In this example the data set accessed by the both references to array A form a two dimensional set while the data sets accessed through each reference are one dimensional. The data accessed through both references overlap in A i i . In the exemplary embodiment a piecewise transformation of A is applied which separates A into two subsets one for each one dimensional data set and marks one of them as receiving the updates let us call it the writing reference to the duplicated data. In the example the duplicated data is A i i and the iteration domain is partitioned into three polyhedral domains 0 j

In other exemplary embodiments the geometric rearrangement is a piecewise affine transformation that defines a partition of the iteration domain and of the data sets in such a way that the number of references to a local array varies from one element of the partition to another. In the following example in which the possible values of variable i are 0 i 99900 the data sets accessed through reference A j and A i j overlap when i is less than 100. Otherwise they do not overlap.

Since those data sets overlap for some values of i both references are put in the same group of compatible references. If the accessed data sets are allocated as a single local array the amount of memory necessary to contain the array is 10000 memory cells. On the other hand if they are allocated as two separate arrays some of the data would have to be duplicated and the iteration domain the j loop here would have to be partitioned as in the previous exemplary embodiment. The amount of overlap when i is less than 100 may not be small enough and it may not be profitable to perform the duplication. The geometric rearrangement provided by the embodiment is a piecewise affine transformation that defines a partition of the set of parameters in the current example i A1 j A j for 0 i

One advantage of the geometric rearrangement that is performed by this exemplary embodiment is that the j loops are not partitioned. Partitioning the loops into smaller loops is often a factor of performance degradation which is avoided in this exemplary embodiment. The partition of i is obtained by computing the domain in which both data sets intersect by projecting the intersection onto the vector space of the parameters in the current example the parameter is i and the projected domain is i

The operation flow of a further provided embodiment of a local memory compaction module is illustrated in . In this embodiment flow begins at block where source code is received in memory. Similar to the above embodiment the source code represents loops with arbitrary parametric affine iteration domains and contain at least one array reference. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. In this embodiment the identification of inefficiencies includes block where strides in the polyhedral domain that defines the accessed dataset are identified and block where a lattice of integer points within the domain is extracted from the domain. These integer points indicate that only a regular subset of the accessed data region is accessed. In this manner more efficient allocation of local arrays is accomplished because portions of the array that are not accessed are identified and excluded from the mapping from the array to the local array.

An additional provided embodiment is illustrated in . In this embodiment like earlier embodiments flow begins at block where source code is received in memory. Similar to the above embodiment the source code represents loops with arbitrary parametric affine iteration domain and contains at least one array reference. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. Flow then continues to block where at least one local array is allocated and in block a portion of the array with inefficient memory usage is mapped into the local array. In this embodiment like in the embodiment illustrated by mapping block includes partitioning references to form compatible references in block determining a relation within compatible references in block grouping compatible references based on the relation in block performing algebraic simplification in block and performing geometric arrangement in block . The algebraic simplification block includes block where a representative array reference is extracted from a set of references accessing a portion of the array. In some embodiments the representative array reference represents a set of references which access polyhedral datasets whose accessed points all lie on a lattice of integer points that is not the standard lattice on which any integer point lies. These embodiments take advantage of the fact that array references represent affine functions which can be represented as matrices called access matrices . In the exemplary embodiment the flow within block goes from block to block where a Hermite factorization is performed for the access matrix representing the representative array reference. The Hermite factorization produces a piecewise affine index function.

One purpose of Hermite factorization is to reduce the dimension of the reference to the actual geometric dimension of the data footprint. In addition if the access pattern contains strides i.e. regular intervals between accessed data using the non unimodular matrix that results from the Hermite factorization in the transformation removes these strides in the resulting local references. For example given an affine access function f x y on loop indices x and parameters y we first decompose it into the sum of g x h y where g x is a linear function on x and h y is an affine function on y. This decomposition is an algebraic simplification that makes it possible to perform further computations on the part of f x y that involves variables only. Function g x can be decomposed into g x HU where H H 0 is the Hermite Normal Form of g x and U is unimodular matrix. Let

Hermite factorizations have many uses as lattice computations. The Hermite factorization of a matrix G written G HU writes matrix G as the product of two matrices H and U. H called the Hermite normal form is a canonical representation of the lattice also represented by G. U is a unimodular matrix which entails that U when used as a transformation always transforms any point that has integer coordinates into another point that has integer coordinates. Also any point that has integer coordinates can be obtained by transforming a point with integer coordinates using a unimodular transformation. This is important since most programming language conventions enforce that data elements and particularly array elements must have integer coordinates.

The flow of a still further provided embodiment is illustrated in . In this embodiment like previous embodiments flow begins at block where source code is received in memory. Similar to the above embodiment the source code represents loops with arbitrary parametric affine iteration domain and contain at least one array reference. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. Flow then continues to block where at least one local array is allocated and in block a portion of the array with inefficient memory usage is mapped into the local array. In this embodiment mapping block includes partitioning references to form compatible references in block determining a relation within compatible references in block grouping compatible references based on the relation in block performing algebraic simplification in block and performing geometric arrangement in block . Geometric rearrangement contains blocks where linear constraints are formed block where sets of linear programming problems are formed from the linear constraints and solved and block where a data reindexing is computed. In some embodiments the flow goes back to block . In such embodiments geometric rearrangements are applied iteratively until no reindexing function is found that reduces memory requirements.

Most modern programming languages abide by the convention that multi dimensional arrays are allocated in memory as if they were canonical rectangular parallelotopes. In a space of d dimensions a parallelotope is a finite polyhedron defined by 2d faces and whose faces are pair wise parallel. A canonical rectangular parallelotope is a parallelotope for which the normal vectors to its faces are either a canonical vector or the negation of a canonical vector. Examples of rectangular parallelotopes are a cube in a 3 dimensional space and a rectangle in a 2 dimensional space . In an exemplary embodiment the transformation is a unimodular reindexing of the accessed data that minimizes the size of the smallest canonical rectangular parallelotope that encloses the accessed dataset. The smaller the enclosing rectangular parallelotope the smaller the amount of memory that has to be allocated for the dataset.

In some embodiments this is accomplished by formulating a first set of linear constraints through the use of Farkas Lemma. This first set of linear programming constraints is decomposed dimension by dimension to form a set of integer linear programming problems. This set of problems is then solved to provide the data reindexing function which can then be applied to the at least one local array. Unimodular reindexings transform integer points into integer points. Hence the convention that data elements have integer coordinates is preserved by such a reindexing. In the case of affine transformations the linear part of the transformation can be represented by a unimodular matrix.

Farkas lemma is a basic linear algebra theorem which is often used to obtain from a set of affine constraints i.e. inequalities and equalities on variables with unknown coefficients constraints that apply to the unknown coefficient themselves. In this embodiment it is used to obtain a set of constraints involving the coefficients of the unimodular data reindexing function which is represented as a matrix and the width of the enclosing rectangular parallelotope along each dimension. From those obtained constraints the method embodiment finds values of the coefficients of the unimodular data reindexing function for which the width is minimal using integer linear programming. For example the data set accessed through reference B i j j in the following pseudo code can be reindexed so as to occupy only 100 memory cells 

The coordinates x x of the elements of array B accessed by that loop node are defined by the constraints D n x

An integer linear programming problem defines a linear function of a set of variables called the objective function and whose minimal or alternatively maximal value over a polyhedral domain called the feasible set is looked for. Solvers for such problems typically return a polyhedral domain within the feasible set for which the value of the objective function is minimal. In the running example the embodiment finds 

In one of the exemplary embodiments the unimodular nature of the reindexing matrix U is obtained by forcing U to be triangular and forcing the absolute value of the diagonal elements to be one. In another embodiment the unimodular nature of the reindexing matrix is obtained by composition of an upper triangular unimodular and a lower triangular unimodular matrix. The advantage of that other embodiment is that the class of unimodular reindexing functions produced is not limited to the reindexing functions represented by a triangular matrix. Finding those two matrices is equivalent to reindexing data twice first by finding an upper triangular reindexing matrix as described above and applying the reindexing and then by finding a lower triangular reindexing matrix for the reindexed set and by applying that second reindexing. Yet another embodiment produces in the same way a unimodular reindexing by composition of an upper triangular unimodular matrix a permutation matrix and a lower triangular unimodular matrix. The advantage of the embodiment is that the class of reindexing function that can be produced is the whole class of integer unimodular matrices.

Turning to which illustrates another embodiment of a provided method like the previous embodiments flow begins in block where source code is received in memory. Similar to the above embodiment the source code represents loops with arbitrary parametric affine iteration domain and contains at least one array reference. Flow continues to block where inefficiencies in memory usage in the at least one array are identified. Flow then continues to block where at least one local array is allocated and in block a portion of the array with inefficient memory usage is mapped into the local array. In this illustration block contains block where a parallelotope of minimal volume is derived this parallelotope enclosing the domain of the data set accessed by the local arrays. Block additionally contains block where a finite prism of triangular base is derived.

As used herein a finite prism is a polyhedron defined by a set of translations of a base polyhedron which lies in a subspace of the considered space by a finite convex set of linear combinations of vectors of the complementary subspace. Since they are finite it is possible to characterize the maximum extent of a finite prism along the directions of the complementary subspace. In this document those extents are called height of the prism there is one height along every direction of the complementary subspace . A triangular prism is a prism whose base polyhedron is a triangle. In two dimensions it is just a triangle. In one embodiment this finite prism has a minimum volume that encloses the data footprint domain. In block the prism is compared to the parallelotope. In block the prism is partitioned into two prisms. One of the two is then transformed using a central symmetry such that the union of the transformed prism and the non transformed prism has a smaller memory footprint than the enclosing parallelotope. One advantage of that embodiment is that it provides data layouts that have smaller memory requirements for a class of accessed datasets for which methods based on parallelotopes are not optimal.

For instance the dataset accessed by the program represented by the following pseudo code through reference B is triangular 

The embodiment finds three constraints that enclose the accessed data set in a similar way as in the embodiment depicted in using the Farkas lemma. The minimal volume for a parallelotope that encloses the dataset would be about twice the volume of the triangle. Hence using such a parallelotope to determine the memory allocation of the dataset is bound to be sub optimal. Instead the current embodiment depicted in defines a tighter enclosing polyhedron using three inequalities it is then a prism of triangular base . Using the enclosing prism the data set is partitioned in two subsets say A and B and subset A is re indexed in such a way that both the array elements in B and the re indexed elements are enclosed in a smaller parallelotope than the original parallelotope. The volume of the new parallelotope is about the volume of the prism of triangular base. Since there is a parallelotope of smaller volume enclosing the reindexed data set its memory requirements are smaller. The result is a piecewise affine array reindexing which typically partitions the loop iterations into the iterations that access A and the ones that access B.

In the current embodiment the three inequalities a aI a 0 b bI b 0 c cI c0 0 that define the triangular prism P where I is the vector of data coordinates are used to devise the partitioning. Let xa point in the intersection of b and c and let w axI a. The prism is partitioned into A and B as follows 

The tightest enclosing triangle defined by 0 x1 0 x2 x1 x2 9 by comparison includes 55 elements which is about half the number of elements required for the enclosing parallelotope. Since the number of array elements in the enclosing triangle is less than the number of array elements in the enclosing parallelotope the embodiment considers the tightest enclosing triangle and partitions the enclosed data into data subsets A 0 x 5 x x x 9 and B 0 x 0 x 4 x x 9. Point x 5 9 2 is selected as center of symmetry and the elements of A are then transformed into a new array subset A as follows 

Many computers that contain processors that have an explicitly managed local memory also have the ability to transfer data at the same time as they are performing other computations. Such transfers are called asynchronous . The main reason for using that feature is that the typical time necessary for such transfers is often comparable to the time taken to perform computations between two consecutive transfers of input data. Since doing both transfer and computation at the same time takes less time than doing one after another the effect of overlapping them is to improve the overall program execution time. The use of several memory zones specialized to either execution reception or sending of data makes the overlap possible. Such a use is called multi buffering . The specialization of the buffers is also modified at certain times. Such a modification is called a rotation of the buffers since a buffer is cyclically assigned the same specialization.

One embodiment computes a local memory mapping adds a polyhedral representation of the communications and schedules communications and computations in a multi buffering scheme for the program represented by the following pseudo code. In this pseudo code every iteration of the k loop works on a distinct instance of local memory 

Illustrated in are computing apparatus and computer software products consistent with provided embodiments. Computing apparatus includes processor memory storage medium and in some embodiments input port and network interface . In many provided embodiments storage medium contains a set of processor executable instructions that when executed by processor configure computing apparatus to implement the modules and methods described herein. In one embodiment storage medium containing the set of processor executable instructions resides in another computing apparatus across network . In an embodiment of a computer software product computer software product is a computer readable storage medium containing processor executable instructions sufficient that when executed by processor configure computing apparatus to implement the above described modules and methods. Further computer software product in some embodiments consists of a physical medium configured to interface with input port to allow its contents to be copied to storage medium . In other embodiments computer software product is an internal storage medium such as . An additional embodiment of computing apparatus includes a plurality of processors a plurality of memories a storage medium and in some embodiments input port and network connection . In some embodiments one or more processors is a host while others are modeled in the form of a grid.

Thus it is seen that methods apparatus and computer software products for allocating arrays in memories with constrained memory requirements according to the way those arrays are accessed is provided. One skilled in the art will appreciate that the present invention can be practiced by other than the above described embodiments which are presented in this description for purposes of illustration and not of limitation. The specification and drawings are not intended to limit the exclusionary scope of this patent document. It is noted that various equivalents for the particular embodiments discussed in this description may practice the invention as well. That is while the present invention has been described in conjunction with specific embodiments it is evident that many alternatives modifications permutations and variations will become apparent to those of ordinary skill in the art in light of the foregoing description. Accordingly it is intended that the present invention embrace all such alternatives modifications and variations as fall within the scope of the appended claims. The fact that a product process or method exhibits differences from one or more of the above described exemplary embodiments does not mean that the product or process is outside the scope literal scope and or other legally recognized scope of the following claims.

