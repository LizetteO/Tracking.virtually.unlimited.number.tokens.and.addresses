---

title: System and method for probabilistic relational clustering
abstract: Relational clustering has attracted more and more attention due to its phenomenal impact in various important applications which involve multi-type interrelated data objects, such as Web mining, search marketing, bioinformatics, citation analysis, and epidemiology. A probabilistic model is presented for relational clustering, which also provides a principal framework to unify various important clustering tasks including traditional attributes-based clustering, semi-supervised clustering, co-clustering and graph clustering. The model seeks to identify cluster structures for each type of data objects and interaction patterns between different types of objects. Under this model, parametric hard and soft relational clustering algorithms are provided under a large number of exponential family distributions. The algorithms are applicable to relational data of various structures and at the same time unify a number of state-of-the-art clustering algorithms: co-clustering algorithms, the k-partite graph clustering, and semi-supervised clustering based on hidden Markov random fields.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08285719&OS=08285719&RS=08285719
owner: The Research Foundation of State University of New York
number: 08285719
owner_city: Binghamton
owner_country: US
publication_date: 20090810
---
This invention was made with government support under award 11S0535162 awarded by the National Science Foundation and FA8750 05 2 0284 awarded by AFRL IF and award FA9550 06 1 0327 awarded by USAF AFOSR. The government has certain rights in this invention.

A portion of the work described herein was supported in part by NSF 115 0535162 AFRL FA8750 05 2 0284 AFOSR FA9550 06 1 0327 . The U.S. Government may have certain rights in this invention.

Most clustering approaches in the literature focus on flat data in which each data object is represented as a fixed length attribute vector 38 . However many real world data sets are much richer in structure involving objects of multiple types that are related to each other such as documents and words in a text corpus Web pages search queries and Web users in a Web search system and shops customers suppliers shareholders and advertisement media in a marketing system.

In general relational data contain three types of information attributes for individual objects homogeneous relations between objects of the same type heterogeneous relations between objects of different types. For example for a scientific publication relational data set of papers and authors the personal information such as affiliation for authors are attributes the citation relations among papers are homogeneous relations the authorship relations between papers and authors are heterogeneous relations. Such data violate the classic independently and identically distributed IID assumption in machine learning and statistics and present huge challenges to traditional clustering approaches. An intuitive solution is that we transform relational data into flat data and then cluster each type of objects independently. However this may not work well due to the following reasons.

First the transformation causes the loss of relation and structure information 14 . Second traditional clustering approaches are unable to tackle influence propagation in clustering relational data i.e. the hidden patterns of different types of objects could affect each other both directly and indirectly pass along relation chains . Third in some data mining applications users are not only interested in the hidden structure for each type of objects but also interaction patterns involving multi types of objects. For example in document clustering in addition to document clusters and word clusters the relationship between document clusters and word clusters is also useful information. It is difficult to discover such interaction patterns by clustering each type of objects individually.

Moreover a number of important clustering problems which have been of intensive interest in the literature can be viewed as special cases of relational clustering. For example graph clustering partitioning 7 42 13 6 20 28 can be viewed as clustering on singly type relational data consisting of only homogeneous relations represented as a graph affinity matrix co clustering 12 2 which arises in important applications such as document clustering and micro array data clustering can be formulated as clustering on bi type relational data consisting of only heterogeneous relations.

Recently semi supervised clustering 46 4 has attracted significant attention which is a special type of clustering using both labeled and unlabeled data. Therefore relational data present not only huge challenges to traditional unsupervised clustering approaches but also great need for theoretical unification of various clustering tasks.

Clustering on a special case of relational data bi type relational data consisting of only heterogeneous relations such as the word document data is called co clustering or bi clustering. Several previous efforts related to co clustering are model based 22 23 . Spectral graph partitioning has also been applied to bi type relational data 11 25 . These algorithms formulate the data matrix as a bipartite graph and seek to find the optimal normalized cut for the graph.

Due to the nature of a bipartite graph these algorithms have the restriction that the clusters from different types of objects must have one to one associations. Information theory based co clustering has also attracted attention in the literature. 12 proposes a co clustering algorithm to maximize the mutual information between the clustered random variables subject to the constraints on the number of row and column clusters. A more generalized co clustering framework is presented by 2 wherein any Bregman divergence can be used in the objective function. Recently co clustering has been addressed based on matrix factorization. 35 proposes an EM like algorithm based on multiplicative updating rules.

Graph clustering partitioning clusters homogeneous data objects based on pairwise similarities which can be viewed as homogeneous relations. Graph partitioning has been studied for decades and a number of different approaches such as spectral approaches 7 42 13 and multilevel approaches 6 20 28 have been proposed. Some efforts 17 43 21 21 1 based on stochastic block modeling also focus on homogeneous relations. Compared with co clustering and homogeneous relation based clustering clustering on general relational data which may consist of more than two types of data objects with various structures has not been well studied in the literature. Several noticeable efforts are discussed as follows. 45 19 extend the probabilistic relational model to the clustering scenario by introducing latent variables into the model these models focus on using attribute information for clustering. 18 formulates star structured relational data as a star structured m partite graph and develops an algorithm based on semi definite programming to partition the graph. 34 formulates multi type relational data as K partite graphs and proposes a family of algorithms to identify the hidden structures of a k partite graph by constructing a relation summary network to approximate the original k partite graph under a broad range of distortion measures.

The above graph based algorithms do not consider attribute information. Some efforts on relational clustering are based on inductive logic programming 37 24 31 . Based on the idea of mutual reinforcement clustering 51 proposes a framework for clustering heterogeneous Web objects and 47 presents an approach to improve the cluster quality of interrelated data objects through an iterative reinforcement clustering process. There are no sound objective function and theoretical proof on the effectiveness and correctness convergence of the mutual reinforcement clustering. Some efforts 26 50 49 5 in the literature focus on how to measure the similarities or choosing cross relational attributes.

To summarize the research on relational data clustering has attracted substantial attention especially in the special cases of relational data. However there is still limited and preliminary work on general relational data clustering.

Most clustering approaches in the literature focus on flat data in which each data object is represented as a fixed length attribute vector. However many real world data sets are much richer in structure involving objects of multiple types that are related to each other such as documents and words in a text corpus Web pages search queries and Web users in a Web search system and shops customers suppliers share holders and advertisement media in a marketing system.

In general relational data contain three types of information attributes for individual objects homogeneous relations between objects of the same type heterogeneous relations between objects of different types. For example for a scientific publication relational data sets of papers and authors the personal information such as affiliation for authors are attributes the citation relations among papers are homogeneous relations the authorship relations between papers and authors are heterogeneous relations. Such data violate the classic IID assumption in machine learning and statistics and present significant challenges to traditional clustering approaches. An intuitive solution is that relational data is transformed into flat data and then each type of object clustered independently. However this may not work well due to the following reasons. First the transformation causes the loss of relation and structure information. Second traditional clustering approaches are unable to tackle influence propagation in clustering relational data i.e. the hidden patterns of different types of objects could affect each other both directly and indirectly pass along relation chains . Third in some data mining applications users are not only interested in the hidden structure for each type of objects but also interaction patterns involving multi types of objects. For example in document clustering in addition to document clusters and word clusters the relationship between document clusters and word clusters is also useful information. It is difficult to discover such interaction patterns by clustering each type of objects individually.

Moreover a number of important clustering problems which have been of intensive interest in the literature can be viewed as special cases of relational clustering. For example graph clustering partitioning can be viewed as clustering on singly type relational data consisting of only homogeneous relations represented as a graph affinity matrix co clustering which arises in important applications such as document clustering and micro array data clustering can be formulated as clustering on bi type relational data consisting of only heterogeneous relations.

Recently semi supervised clustering has attracted significant attention which is a special type of clustering using both labeled and unlabeled data. It can be formulated as clustering on single type relational data consisting of attributes and homogeneous relations.

The present system and method is based on a probabilistic model for relational clustering which also provides a principal framework to unify various important clustering tasks including traditional attributes based clustering semi supervised clustering co clustering and graph clustering. The model seeks to identify cluster structures for each type of data objects and interaction patterns between different types of objects. It is applicable to relational data of various structures. Under this model parametric hard and soft and hybrid relational clustering algorithms are presented under a large number of exponential family distributions.

There are three main advantages 1 the technique is applicable to various relational data from various applications 2 It is capable of adapting different distribution assumptions for different relational data with different statistical properties and 3 The resulting parameter matrices provide an intuitive summary for the hidden structure for the relational data.

A probabilistic model is herein proposed for relational clustering which also provides a principal framework to unify various important clustering tasks including traditional attributes based clustering semi supervised clustering co clustering and graph clustering. The proposed model seeks to identify cluster structures for each type of data objects and interaction patterns between different types of objects. It is applicable to relational data of various structures. Under this model parametric hard and soft relational clustering algorithms are provided under a large number of exponential family distributions. The algorithms are applicable to various relational data from various applications and at the same time unify a number of state of the art clustering algorithms co clustering algorithms the k partite graph clustering Bregman k means and semi supervised clustering based on hidden Markov random fields.

With different compositions of three types of information attributes homogeneous relations and heterogeneous relations relational data could have very different structures. show three examples of the structures of relational data. refers to a simple bi type of relational data with only heterogeneous relations such as word document data. represents a bi type data with all types of information such as actor movie data in which actors type 1 have attributes such as gender actors are related to each other by collaboration in movies homogeneous relations actors are related to movies type 2 by taking roles in movies heterogeneous relations . represents the data consisting of companies customers suppliers share holders and advertisement media in which customers type 5 have attributes.

A relational data set as a set of matrices is reprsented. Assume that a relational data set has m different types of data objects x x . . . xx where ndenotes the number of objects of the jth type and xdenotes the name of the pobject of the jtype. The observations of the relational data are represented as three sets of matrices attribute matrices F where ddenotes the dimension of attributes for the jtype objects and Fdenotes the attribute vector for object x homogeneous relation matrices S where Sdenotes the relation between xand x heterogeneous relation matrices R where Rdenotes the relation between xand x. The above representation is a general formulation. In real applications not every type of object has attributes homogeneous relations and heterogeneous relations. For example the relational data set in is represented by only one heterogeneous matrix R and the one in is represented by three matrices F Sand R. Moreover for a specific clustering task not use all available attributes and relations are used after feature or relation selection pre processing.

Mixed membership models which assume that each object has mixed membership denoting its association with classes have been widely used in the applications involving soft classification 16 such as matching words and pictures 39 race genetic structures 39 48 and classifying scientific publications 15 .

A relational mixed membership model is provided to cluster relational data referred to as mixed membership relational clustering or MMRC .

Assume that each type of objects Xklatent classes. The membership vectors for all the objects in Xare represented as a membership matrix 0 1 such that the sum of elements of each column is and denotes the membership vector for object x i.e. denotes the probability that object xassociates with the glatent class. The parameters of distributions to generate attributes homogeneous relations and heterogeneous relations are expressed in matrix forms. Let denote the distribution parameter matrix for generating attributes Fsuch that denotes the parameter vector associated with the glatent class. Similarly denotes the parameter matrix for generating homogeneous relations S denotes the parameter matrix for generating heterogeneous relations R. In summary the parameters of MMRC model are .

In general the meanings of the parameters and depend on the specific distribution assumptions. However in Section 4.1 it is shown that for a large number of exponential family distributions these parameters can be formulated as expectations with intuitive interpretations.

Next the latent variables are introduced into the model. For each object x a latent cluster indicator vector is generated based on its membership parameter which is denoted as C i.e. C 0 1is a latent indicator matrix for all the jtype objects in X.

In the above generative process a latent indicator vector for each object is generated based on multinomial distribution with the membership vector as parameters. Observations are generated independently conditioning on latent indicator variables. The parameters of condition distributions are formulated as products of the parameter matrices and latent indicators i.e. and .

Under this formulation an observation is sampled from the distributions of its associated latent classes. For example if Cindicates that xis with the glatent class and Cindicates that xis with the hlatent class then C C . Hence Pr R implies that the relation between xand xis sampled by using the parameter .

With matrix representation the joint probability distribution over the observations and the latent variables can be formulated as follows 

In this section the parametric soft and hard relational clustering algorithms based on the MMRC model are derived under a large number of exponential family distributions.

To avoid clutter instead of general relational data relational data similar to the one in may be employed which is a representative relational data set containing all three types of information for relational data attributes homogeneous relations and heterogeneous relations. However the derivation and algorithms are applicable to general relational data. For the relational data set in there are two types of objects one attribute matrix F one homogeneous relation matrix S and one heterogeneous relation matrix R. Based on Eq. 1 we have the following likelihood function 2 

For the likelihood function in Eq. 2 the specific forms of condition distributions for attributes and relations depend on specific applications. Presumably for a specific likelihood function a specific algorithm should be derived. However a large number of useful distributions such as normal distribution Poisson distribution and Bernoulli distributions belong to exponential families and the distribution functions of exponential families can be formulated as a general form. This advantageous property facilitates derivation of a general EM algorithm for the MMRC model.

It is shown in the literature 3 9 that there exists bijection between exponential families and Bregman divergences 40 . For example the normal distribution Bernoulli distribution multinomial distribution and exponential distribution correspond to Euclidean distance logistic loss KL divergence and Itakura Satio distance respectively. Based on the bijection an exponential family density Pr x can always be formulated as the following expression with a Bregman divergence D exp 3 where x is a uniquely determined function for each exponential probability density and is the expectation parameter. Therefore for the MMRC model under exponential family distributions exp 4 exp 5 exp 6 

In the above equations a Bregman divergence of two matrices is defined as the sum of the Bregman divergence of each pair of elements from the two matrices. Another advantage of the above formulation is that under this formulation the parameters and are expectations of intuitive interpretations. consists of center vectors of attributes provides an intuitive summary of cluster structure within the same type objects since implies expectation relations between the gcluster and the hcluster of type 1 objects similarly provides an intuitive summary for cluster structures between the different type objects. In the above formulation different Bregman divergences are used D D and D for the attributes homogeneous relations and heterogeneous relations since they could have different distributions in real applications. For example suppose

Substituting Eqs. 4 5 6 7 and 8 into Eq 2 and taking some algebraic manipulations the following log likelihood function is obtained for MMRC under exponential families 

Expectation Maximization EM is a general approach to find the maximum likelihood estimate of the parameters when the model has latent variables. EM does maximum likelihood estimation by iteratively maximizing the expectation of the complete log likelihood which is the following under the MMRC model tilde over log tilde over 10 where tilde over denotes the current estimation of the parameters and is the new parameters that we optimize to increase Q. Two steps E step expectation step and M step minimization step are alternatively performed to maximize the objective function in Eq. 10 .

There exist several techniques for computing intractable posterior such as Monte Carlo approaches belief propagation and variational methods. A Monte Carlo approach Gibbs sampler is further analyzed which is a method of constructing a Markov chain whose stationary distribution is the distribution to be estimated. It is of course understood that other known techniques may be employed.

It is relatively easy to compute the posterior of a latent indicator vector while fixing all other latent indicator vectors i.e. 

Note that at each sampling step in the above procedure the latent indicator variables sampled from previous steps are used. The above procedure iterates until the stop criterion is satisfied. It can be shown that the above procedure is a Markov chain converging to Pr C C F S R tilde over . Assume that we keep l samples for estimation then the posterior can be obtained simply by the empirical joint distribution of Cand Cin the l samples.

After the E step the posterior probability of latent variables is available to evaluate the expectation of the complete log likelihood 

First the update rules for membership parameters and are derived. To derive the expression for each the Lagrange multiplier is introduced with the constraint 1 and the following equation solved 

Summing both sides over h 1 is obtained resulting in the following update rule 1 17 i.e. is updated as the posterior probability that the pobject is associated with the hcluster. Similarly the following update rule for is provided 1 18 

Second the update rule for is derived. Based on Eqs. 9 and 13 optimizing is equivalent to the following optimization 

To solve the above optimization an important property of Bregman divergence presented in the following theorem may be used.

Let X be a random variable taking values in x x Sfollowing v. Given a Bregman divergence D S int S 0 the problem

The proof of Theorem 1 is omitted please refer 3 40 . Theorem 1 states that the Bregman representative of a random variable is always the expectation of the variable. Based on Theorem 1 and the objective function in 20 we update as follows 

Third the update rule for is derived. Based on Eqs. 9 and 13 optimizing is formulated as the following optimization 

Fourth the update rule for is derived. Based on Eqs. 9 and 13 optimizing is formulated as the following optimization 

Combining the E step and M step a general relational clustering algorithm is provided Exponential Family MMRC EF MMRC algorithm which is summarized in Algorithm 1. Since it is straightforward to apply the algorithm derivation to a relational data set of any structure Algorithm 1 is proposed based on the input of a general relational data set. Despite that the input relational data could have various structures EF MMRC works simply as follows in the E step EF MMRC iteratively updates the posterior probabilities that an object is associated with the clusters the Markov chain in Section 4.2 in the M step based on the current cluster association posterior probabilities the cluster representatives of attributes and relations are updated as the weighted mean of the observations no matter which exponential distributions are assumed.

Therefore with the simplicity of the traditional centroid based clustering algorithms EF MMRC is capable of making use of all attribute information and homogeneous and heterogenous relation information to learn hidden structures from various relational data. Since EF MMRC simultaneously clusters multi type interrelated objects the cluster structures of different types of objects may interact with each other directly or indirectly during the clustering process to automatically deal with the influence propagation. Besides the local cluster structures for each type of objects the output of EF MMRC also provides the summary of the global hidden structure for the data i.e. based on and we know how the clusters of the same type and different types are related to each other. Furthermore relational data from different applications may have different probabilistic distributions on the attributes and relations it is easy for EF MMRC to adapt to this situation by simply using different Bregman divergences corresponding to different exponential family distributions.

If we assume O m types of heterogeneous relations among m types of objects which is typical in real applications and let n n and k k the computational complexity of EF MMRC can be shown to be O tmnk for t iterations. If the k means algorithm are applied to each type of nodes individually by transforming the relations into attributes for each type of nodes the total computational complexity is also O tmnk .

Due to its simplicity scalability and broad applicability k means algorithm has become one of the most popular clustering algorithms. Hence it is desirable to extend k means to relational data. Some efforts 47 2 12 33 in the literature work in this direction. However these approaches apply to only some special and simple cases of relational data such as bi type heterogeneous relational data.

As traditional k means can be formulated as a hard version of Gaussian mixture model EM algorithm 29 the hard version of MMRC algorithm is presented as a general relational k means algorithm Algorithm 1 is herein referred to as soft EF MMRC which applies to various relational data.

To derive the hard version MMRC algorithm soft membership parameters are omitted in the MMRC model Cin the model provides the hard membership for each object . Next the computation of the posterior probabilities in the E step is changed to a reassignment procedure i.e. in the E step based on the estimation of the current parameters cluster labels C are reassigned to maximize the objective function in 9 . In particular for each object while fixing the cluster assignments of all other objects each cluster is assigned to find the optimal cluster assignment maximizing the objective function in 9 which is equivalent to minimizing the Bregman distances between the observations and the corresponding expectation parameters. After all objects are assigned the re assignment process is repeated until no object changes its cluster assignment between two successive iterations.

In the M step the parameters are estimated based on the cluster assignments from the E step. A simple way to derive the update rules is to follow the derivation in Section 4.3 but replace the posterior probabilities by its hard versions. For example after the E step if the object xis assigned to the gcluster i.e. C 1 then the posterior Pr C 1 F S R tilde over 1 and Pr C 1 F S R tilde over 0 for h g.

In the above update rule since Cis the size of the gcluster is actually updated as the mean of the attribute vectors of the objects assigned to the gcluster. Similarly the following update rule 

Each heterogeneous relation expectation parameter is updated as the mean of the objects of the itype from the gcluster and of the jtype from the hcluster 

The hard version of EF MMRC algorithm is summarized in Algorithm 2. It works simply as the classic k means. However it is applicable to various relational data under various Bregman distance functions corresponding to various assumptions of probability distributions. Based on the EM framework its convergence is guaranteed. When applied to some special cases of relational data it provides simple and new algorithms for some important data mining problems. For example when applied to the data of one homogeneous relation matrix representing a graph affinity matrix it provides a simple and new graph partitioning algorithm.

Based on Algorithms 1 and 2 there is another version of EF MMRC i.e. soft and hard EF MMRC may be combined together to have mixed EF MMRC. For example hard EF MMRC may be run several times as initialization then soft EF MMRC run.

The connections between existing clustering approaches and the MMRF model and EF MMRF algorithms are now discussed. By considering them as special cases or variations of the MMRF model MMRF is shown to provide a unified view to the existing clustering approaches from various important data mining applications.

Recently semi supervised clustering has become a topic of significant interest 4 46 which seeks to cluster a set of data points with a set of pairwise constraints.

Semi supervised clustering can be formulated as a special case of relational clustering clustering on the single type relational data set consisting of attributes F and homogeneous relations S. For semi supervised clustering Sdenotes the pairwise constraint on the pth object and the qth object.

 4 provides a general model for semi supervised clustering based on Hidden Markov Random Fields HMRFs . It can be formulated as a special case of MMRC model. As in 4 the homogeneous relation matrix S can be defined as follows 

The above likelihood function is equivalent to the objective function of semi supervised clustering based on HMRFs 4 . Furthermore when applied to optimizing the objective function in Eq. 32 hard MMRC provides a family of semi supervised clustering algorithms similar to HMRF K Means in 4 on the other hand soft EF MMRC provides new and soft version semi supervised clustering algorithms.

Co clustering or bi clustering arise in many important applications such as document clustering micro array data clustering. A number of approaches 12 8 33 2 have been proposed for co clustering. These efforts can be generalized as solving the following matrix approximation problem 34 

Co clustering is equivalent to clustering on relational data of one heterogeneous relation matrix R. Based on Eq. 9 by omitting the soft membership parameters maximizing log likelihood function of hard clustering on a heterogeneous relation matrix under the MMRC model is equivalent to the minimization in 33 . The algorithms proposed in 12 8 33 2 can be viewed as special cases of hard EF MMRC. At the same time soft EF MMRC provides another family of new algorithms for co clustering.

 34 proposes the relation summary network model for clustering k partite graphs which can be shown to be equivalent on clustering on relational data of multiple heterogeneous relation matrices. The proposed algorithms in 34 can also be viewed as special cases of the hard EF MMRC algorithm.

Graph clustering partitioning is an important problem in many domains such as circuit partitioning VLSI design task scheduling. Existing graph partitioning approaches are mainly based on edge cut objectives such as Kernighan Lin objective 30 normalized cut 42 ratio cut 7 ratio association 42 and min max cut 13 .

Graph clustering is equivalent to clustering on single type relational data of one homogeneous relation matrix S. The log likelihood function of the hard clustering under MMRC model is D S C C . We propose the following theorem to show that the edge cut objectives are mathematically equivalent to a special case of the MMRC model. Since most graph partitioning objective functions use weighted indicator matrix such that CC I where Iis an identity matrix we follow this formulation in the following theorem.

With restricting to be the form of rIfor r 0 maximizing the log likelihood of hard MMRC clustering on S under normal distribution i.e. 

The above deduction uses the property of trace tr XY tr YX . Since tr SS r and k are constants the maximization of L is equivalent to the maximization of tr CSC .

Since it is shown in the literature 10 that the edge cut objectives can be formulated as the trace maximization Theorem 2 states that edge cut based graph clustering is equivalent to MMRC model under normal distribution with the diagonal constraint on the parameter matrix . This connection provides not only a new understanding for graph partitioning but also a family of new algorithms soft and hard MMRC algorithms for graph clustering.

Finally we point out that MMRC model does not exclude traditional attribute based clustering. When applied to an attribute data matrix under Euclidean distances hard MMRC algorithm is actually reduced to the classic k means soft MMRC algorithm is very close to the traditional mixture model EM clustering except that it does not involve mixing proportions in the computation.

In summary MMRC model provides a principal framework to unify various important clustering tasks including traditional attributes based clustering semi supervised clustering co clustering and graph clustering soft and hard EF MMRC algorithms unify a number of state of the art clustering algorithms and at the same time provide new solutions to various clustering tasks.

This section provides empirical evidence to show the effectiveness of the MMRC model and algorithms. Since a number of state of the art clustering algorithms 12 8 33 2 3 4 can be viewed as special cases of EF MMRC model and algorithms the experimental results in these efforts also illustrate the effectiveness of the MMRC model and algorithms. MMRC algorithms are applied to tasks of graph clustering bi clustering tri clusering and clustering on a general relational data set of all three types of information. In the experiments mixed version MMRC was employed i.e. hard MMRC initialization followed by soft MMRC. Although MMRC can adopt various distribution assumptions due to space limit MMRC is used under normal or Poisson distribution assumption in the experiments. However this does not imply that they are optimal distribution assumptions for the data. Therefore one can select or derive an optimal distribution assumption as may be appropriate.

For performance measure the Normalized Mutual Information NMI 44 between the resulting cluster labels and the true cluster labels was used which is a standard way to measure the cluster quality. The final performance score is the average of ten runs.

Experiments on the MMRC algorithm are presented under normal distribution in comparison with two representative graph partitioning algorithms the spectral graph partitioning SGP from 36 that is generalized to work with both normalized cut and ratio association and the classic multilevel algorithm METIS 28 .

The graphs based on the text data have been widely used to test graph partitioning algorithms 13 11 25 . In this study we use various data sets from the 20 newsgroups 32 WebACE and TREC 27 which cover data sets of different sizes different balances and different levels of difficulties. The data are pre processed by removing the stop words and each document is represented by a term frequency vector using TF IDF weights. Relational data are then constructed for each text data set such that objects documents are related to each other with cosine similarities between the term frequency vectors. A summary of all the data sets to construct relational data used in this paper is shown in Table 1 in which n denotes the number of objects in the relational data k denotes the number of true clusters and balance denotes the size ratio of the smallest clusters to the largest clusters.

For the number of clusters k the number of the true clusters is used. Determining the optimal number of clusters analytically is a model selection problem otherwise this may be determined empirically or iteratively.

The MMRC algorithm are now applied under Poisson distribution to clustering bi type relational data word document data and tri type relational data word document category data. Two algorithms Bi partite Spectral Graph partitioning BSGP 11 and Relation Summary Network under Generalized I divergence RSN GI 34 are used as comparison in bi clustering. For tri clustering Consistent Bipartite Graph Co partitioning CBGC 18 and RSN GI are used as comparison.

The bi type relational data word document data are constructed based on various subsets of the 20 Newsgroup data. The data is pre processed by selecting the top 2000 words by the mutual information. The document word matrix is based on tf.idf weighting scheme and each document vector is normalized to a unit Lnorm vector. Specific details of the data sets are listed in Table 2. For example for the data set BT NG3 200 documents are randomly and evenly sampled from the corresponding newsgroups then a bi type relational data set of 1600 document and 2000 word is formulated.

The tri type relational data are built based on the 20 newsgroups data for hierarchical taxonomy mining. In the field of text categorization hierarchical taxonomy classification is widely used to obtain a better trade off between effectiveness and efficiency than flat taxonomy classification. To take advantage of hierarchical classification one must mine a hierarchical taxonomy from the data set. We see that words documents and categories formulate a sandwich structure tri type relational data set in which documents are central type nodes. The links between documents and categories are constructed such that if a document belongs to k categories the weights of links between this document and these k category nodes are 1 k please refer 18 for details . The true taxonomy structures for two data sets TP TM1 and TP TM2 are documented in Table 3.

The MMRC algorithm was also run on the actor movie relational data based on IMDB movie data set for a case study. In the data actors are related to each other by collaboration homogeneous relations actors are related to movies by taking roles in movies heterogeneous relations movies have attributes such as release time and rating note that there is no links between movies . Hence the data have all the three types of information. A data set of 20000 actors and 4000 movies is formulated. Experiments were run with k 200. Although there is no ground truth for the data s cluster structure it may be observed that most resulting clusters that are actors or movies of the similar style such as action or tight groups from specific movie serials. For example Table 4 shows cluster 23 of actors and cluster 118 of movies the parameter shows that these two clusters are strongly related to each other. In fact the actor cluster contains the actors in the movie series The Lord of the Rings . Note that if we only have one type of actor objects we only get the actor clusters but with two types of nodes although there are no links between the movies we also get the related movie clusters to explain how the actors are related.

A probabilistic model is formulated for relational clustering which provides a principal framework to unify various important clustering tasks including traditional attributes based clustering semi supervised clustering co clustering and graph clustering. Under this model parametric hard and soft relational clustering algorithms are presented under a large number of exponential family distributions. The algorithms are applicable to relational data of various structures and at the same time unify a number of state of the art clustering algorithms. The theoretic analysis and experimental evaluation show the effectiveness and great potential of the model and algorithms.

The invention is applicable to various relational data from various applications. It is capable of adapting different distribution assumptions for different relational data with different statistical properties. While the above analysis discuss in depth certain types of statistical distributions the system and method may be used with any statistical distribution. The resulting parameter matrices provides an intuitive summary for the hidden structure for relational data. Therefore in addition to finding application in clustering data objects the present system and method may be used for more general analysis of relationships of data for other end purposes and or as an intermediary step in a larger or more complex data analysis paradigm.

The present invention has significant versatility and can be applied to a wide range of applications involving relational data. Examples include but are not limited to 

The present method may be implemented on a general purpose computer or a specially adapted machine. In the former case. Typically a programmable processor will execute machine readable instructions stored on a computer readable medium. In other cases the method will be implemented using application specific hardware and may not be reprogrammable.

An exemplary programmable computing device for implementing an embodiment of the invention includes at least a processing unit and a memory. Depending on the exact configuration and type of computing device the memory may be volatile such as RAM non volatile such as ROM flash memory etc. or some combination of the two. Additionally the device may also have additional features functionality. For example the device may also include additional storage removable and or non removable including but not limited to magnetic or optical disks or tapes. Computer storage media includes volatile and nonvolatile removable and non removable media implemented in any method or technology for storage of information such as computer readable instructions data structures program modules or other data. The memory the removable storage and the non removable storage are all examples of computer storage media. Computer storage media includes but is not limited to RAM ROM EEPROM flash memory FRAM or other memory technology CD ROM digital versatile disks DVD or other optical storage magnetic cassettes magnetic tape magnetic disk storage or other magnetic storage devices or any other medium which can be used to store the desired information and which can accessed by the device. The device may also contain one or more communications connections that allow the device to communicate with other devices. Such communication connections may include for example Ethernet wireless communications optical communications serial busses parallel busses and the like. Communication media typically embodies computer readable instructions data structures program modules or other data in a modulated data signal such as a carrier wave or other transport mechanism and includes any information delivery media. The term modulated data signal means a signal that has one or more of its characteristics set or changed in such a manner as to encode information in the signal. By way of example and not limitation communication media includes wired media such as a wired network or direct wired connection and wireless media such as acoustic RF infrared and other wireless media. As discussed above the term computer readable media as used herein includes both storage media and communication media.

One use for the present method is to process information databases which may be private or public. For example the information database may comprise information received from the Internet such as the content of various web pages from world wide web sites or other information found on the Internet. In other cases the data may be more structured for example the content of the Facebook social networking site system. Further the information may be private user information such as the contents of a user s hard drive especially for example the user generated or downloaded content.

Having described specific embodiments of the present invention it will be understood that many modifications thereof will readily appear or may be suggested to those skilled in the art and it is intended therefore that this invention is limited only by the spirit and scope of the following claims.

