---

title: Three dimensional imaging method and apparatus
abstract: Three-dimensional imaging techniques are used for a visualization method and apparatus. In a preferred embodiment, terrain data is displayed as a series of pixels—areas of terrain elevation data. Individual pixels are analyzed to determine whether they are locally smooth or “warpable” relative to their surrounding neighbor pixels. Those pixels that are locally relatively “smooth,” i.e., those satisfying a given set of criteria, are joined with adjacent neighbor pixels by a process referred to herein as “warping” to create “smooth,” gap-free surfaces. A preferred embodiment includes drawing or generating lines between the centers of two pairs of adjacent pixels to determine a slopes mand mrespectively. The slopes mand mare then analyzed using the following equations/determinations: |m∥≦m; |m∥≦m; and |m−m|≦Δmax; i.e., the slopes mand mmust each be less than or equal to a predetermined threshold mand the difference between the slopes must be less than or equal to a predetermined difference Δmax.
url: http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&Sect2=HITOFF&p=1&u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&r=1&f=G&l=50&d=PALL&S1=08249346&OS=08249346&RS=08249346
owner: The United States of America as represented by the Secretary of the Army
number: 08249346
owner_city: Washington
owner_country: US
publication_date: 20090127
---
This application is a utility application claiming priority of U.S. Provisional Application Ser. No. 61 023 961 filed Jan. 28 2008 entitled A Novel Technique for Visualizing High Resolution 3 D Terrain Maps which is incorporated herein by reference.

The invention described herein may be manufactured used and licensed by or for the United States Government.

Appendix A contains a partial computer program listing adapted for a preferred embodiment of the present invention.

This invention relates to digital terrain elevation techniques and or three dimensional imaging techniques involving scenes landscapes ground areas environmental surroundings indoors and outdoors and the like.

With the arrival of GPS in the 1980 s GPS based surveying technology has made airborne surveying and mapping applications practical. Many have been developed using downward looking lidar instruments mounted in aircraft or satellites. Lidar Light Detection and Ranging is an optical remote sensing technology that uses laser pulses to determine the distance to a target and or gather other information from a target. The distance to an object or target is measured using the time between the transmission of the initial pulse and receipt of the reflected signal. As used herein the term Lidar or LIDAR includes ALSM Airborne Laser Swath Mapping laser altimetry and LADAR Laser Detection and Ranging Lidar differs from radar in that lidar utilizes much shorter wavelengths of the electromagnetic spectrum are used typically in the ultraviolet visible or near infrared. An example of lidar usage is the NASA Experimental Advanced Research Lidar. In general it is possible to image a feature or object only about the same size as the wavelength or larger. The wavelengths are much smaller than radio radar systems and range from about 10 micrometers to the UV ca. 250 nm . At such wavelengths the waves are reflected very well from small objects referred to as backscattering. Different types of scattering are used for different lidar applications most common are Rayleigh scattering Mie scattering and Raman scattering as well as fluorescence.

A laser typically has a very narrow beam which allows the mapping of physical features with very high resolution compared with radar. Suitable combinations of lasers can allow for remote mapping of atmospheric contents by looking for wavelength dependent changes in the intensity of the returned signal.

For example B. L. Stann et al. Intensity modulated diode laser radar using frequency modulation continuous wave ranging techniques Optical Engineering Vol. 35 No. 11 1996 pp. 3270 3278 discloses an adaptation of frequency modulation FM radar ranging principles to an incoherent laser radar LADAR . The LADAR s laser transmitter output is amplitude modulated with a radio frequency subcarrier which itself is linearly frequency modulated. The subcarrier signal may have a start frequency in the tens to low hundreds of megahertz and a stop frequency in the hundreds of megahertz to low gigahertz. The difference between the start and stop frequency F is chosen to establish the desired range resolution R using inter alia the equation R c 2 F where c is the velocity of light. Light reflected from the target is incoherently detected with a photodiode and converted into a voltage waveform which is mixed with an undelayed sample of the original modulation waveform. After clutter is removed the waveform is processed coherently using the discrete Fourier transform to provide target amplitude and range information.

Similarly B. L. Stann in Research Progress on a Focal Plane Array Ladar System Using Chirped Amplitude Modulation Proc. SPIE Laser Radar Technology and Applications VIII Vol. 5086 2003 disclosed the construction of a 32 32 pixel focal plane array FPA LADAR architecture adapted for smart munitions reconnaissance face recognition robotic navigation etc. using chirped amplitude modulation. Ranging of the LADAR architecture was based on a frequency modulation continuous wave technique implemented by directly amplitude modulating a near IR diode laser transmitter with a radio frequency rf subcarrier that was linearly frequency modulated chirped amplitude modulation . The diode s output was collected and projected to form an illumination field in the downrange image area. The returned signal was focused onto an array of optoelectronic mixing metal semiconductor metal detectors where it was detected and mixed with a delayed replica of the laser modulation signal that modulates the responsivity of each detector. The output of each detector was an intermediate frequency IF signal resulting from the mixing process whose frequency was proportional to the target range. Sampling of the IF signal was done continuously over a period of the rf modulation and a signal processor calculated the discrete fast Fourier transform over the IF waveform in each pixel to establish the ranges and amplitudes of all scatterers.

In addition to an overhead view frequently it is beneficial to visualize the terrain from a perspective of an observer within the surrounding area. Three dimensional 3 D terrain mapping technology has been used to provide terrain visualization with recent improvements being made in resolution accuracy quality and amount of area covered. Resolution has increased to 1 m or better and accuracy in absolute world coordinates of better than 1 m is available. Nonetheless the transformation of 3 D maps from low resolutions appropriate only to large areas to high resolutions useful at much smaller scales requires a new approach to terrain visualization. The shortcomings of prior art terrain visualization techniques include those experienced in visualizing data acquired through sensors such as light detection and ranging Lidar devices that have been employed for applications such as detecting and tracking people and vehicles. Imaging techniques have also incorporated or adapted methods for detection of surface variations. When imaging smaller scale scenes conventional devices tend to under sample small scale features such as foliage fences railings and light poles. These features are difficult to identify and distinguish using the commonly employed terrain visualization techniques discussed above. Yet it is important for a visualization to accommodate this under sampling and to differentiate between these under sampled objects and larger smoother objects like vehicles.

Tracking vehicles and people is of great importance for military efforts. 3 D maps can provide important context information for such tracking but visualization at both large and small scales is essential. It is important to not only render particular buildings or road intersections but to meaningfully render the areas immediately surrounding these sites.

One commonly employed technique for terrain visualization involves the generation of single continuous surface. This technique works well for large scale natural features such as mountain ranges or canyons and urban centers dominated by large buildings. Unfortunately this technique falls short when used to visualize terrain data at smaller scales natural features such as bushes and trees can become indistinguishable from small hills or man made objects.

Another commonly available technique for terrain visualization involves the generation of a cloud of points. This technique avoids obscuring the rough nature of small scale natural features such as bushes and trees. Unfortunately these natural features are still difficult to identify because the points generated by large scale features such as the ground and buildings tend to predominate and obscure the points generated by small scale features. Moreover large scale features themselves are inadequately rendered with point clouds because point clouds detract from the solid nature of large scale features.

Examples of conventional three dimensional 3 D terrain maps include maps generated by the Rapid Terrain Visualization RTV program now known as the BuckEye which is run by the Joint Precision Strike Demonstration Project Office to provide rapid generation of digital terrain data to support emerging crisis or contingency operations. The RTV program maps from an aircraft using both laser radar ladar and interferometric synthetic aperture radar sensors. The ladar has higher resolution and produces cleaner and more accurate maps so ladar data is preferred. This sensor measures the terrain elevation by scanning the area with a laser beam and measuring the time it takes the light to travel from the aircraft sensor to the ground and back. For the ladar the program advertises a resolution post spacing of 1 m a vertical accuracy of 15 30 cm and a horizontal accuracy of 30 50 cm. The maps comprise three pieces of information for each 1 mpixel a backscatter intensity value approximately equivalent to a black and white photograph the elevation of the first backscatter return from the laser the highest thing hit and the elevation of the last return the lowest thing hit . For most pixels these two elevations will be the same. But where bushes or trees are present some of the laser energy will be reflected from the top of the trees as a first hit return but some laser energy will also penetrate down to ground to produce the last hit return. The RTV program also provides a fourth derived product that is a color image combining the intensity image with hues that are derived from the elevations.

Another currently available three dimensional imaging software program is 3DEM Software for Terrain Visualization and Flyby Animation found at the website http www.visualizationsoftware.com software.arcgis explorer index.html. According to the website the program will produce three dimensional terrain scenes and flyby animations from a wide variety of freely available data sources including USGS Digital Elevation Model ASCII DEM files USGS Spatial Data Transfer Standard SDTS DEM files NASA Shuttle Radar Topography Mission SRTM files LIDAR Point Cloud LAS files USGS Global 30 Arc Second Elevation Data Set GTOPO30 DEM files NOAA Global Land One km Base Elevation GLOBE DEM files NASA Mars Orbiter Laser Altimeter MOLA files. Any topographic data file organized by rows and columns of elevation data XYZ scattered point topographic data files and terrain data files can be saved in the following formats for use by other GIS programs USGS ASCII Digital Elevation Model .dem GeoTiff Graphics File .tif GeoTiff Digital Elevation Model .tif Binary terrain matrix .bin VRML world .wrl and Terragen terrain .ter . Also according to the website 3DEM can merge multiple DEMs to provide high resolution overhead maps and 3D projections of large surface areas limited only by the computer s memory. Geographic coordinates latitude and longitude are shown on all overhead map displays. Both Lat Lon and UTM coordinates are supported allowing display and measurement of position to high accuracy. Global Positioning System GPS receiver waypoints routes and tracks can be read via serial interface and displayed on 3D images and flybys of the terrain allowing visualization of the path of a trek through the wilderness. 3DEM uses the SGI Microsoft OpenGL libraries for high speed 3D rendering. 3DEM will render 24 bit color three dimensional projections or red blue projections requiring red blue 3D glasses for viewing. 3DEM scenes can be saved in various formats including Windows Bitmap .bmp and jpeg. 3DEM allows low resolution flyby of DEM landscapes using OpenGL. The path through space is recorded in memory during flight allowing subsequent creation of a full resolution mpeg animation along the flight path. Real time flyby animations can be created in the following formats Flyby animation AVI .avi a and Flyby animation MPEG .mpg .mpeg . 3DEM provides an intuitive user interface high reliability and detailed terrain images and flyby animations created from freely available terrain data. 3DEM is a product of Visualization Software LLC by Richard Home. Maps three dimensional terrain images and animations and GPS waypoints and routes produced by the 3DEM computer program are for general visualization purposes only.

Another example of mapping software is disclosed at Http www.esri.com softwar e arcgis explorer index.html. According to the website 

Another terrain software imaging program is the Google Earth type local search and exploration task. In addition to larger scale views to get context one is often interested in areas of a square block or less in the immediate vicinity of one s destination. Such a search is conventionally done with traditional 2 D maps or in a very select few areas with pseudo 3 D maps generated by adding 3 D building models to the 2 D maps.

Aside from the above described need to better images of small scale features such as foliage fences railings and light poles there are many other applications that require smaller size scales. One particularly important application is site surveillance. The objects of site surveillance are often people and the focus of attention is surveillance of a small area around a specific building for a potential intruder. Locations of individual trees and bushes hedges small ravines banks and other natural features become relevant in determining access routes and potential cover. After an alarm they are also important in judging intent and determining the best response. Therefore a visualization technique is required that portrays both small and large features and as well as natural and manmade features. Accordingly there exists a need for a terrain map with increased accuracy in revealing three dimensional aspects of the object or terrain.

A preferred embodiment of the present invention utilizes 3 D ladar generated maps. 3 D terrain maps are available from a number of sources including for example those generated by the Rapid Terrain Visualization RTV program now known as the BuckEye run by the Joint Precision Strike Demonstration Project Office which provides rapid generation of digital terrain data in support of emerging crisis or contingency operations.

The RTV program maps are generated from an aircraft using both laser radar LADAR and interferometric synthetic aperture radar sensors. The LADAR is preferred as it has higher resolution and produces cleaner and more accurate maps. The LADAR sensor measures the terrain elevation by scanning the area with a laser beam and measuring the time it takes the light to travel from the aircraft sensor to the ground and back. For the LADAR sensor the program advertises a resolution post spacing of 1 m a vertical accuracy of 15 30 cm and a horizontal accuracy of 30 50 cm. The maps comprise three pieces of information for each 1 mpixel a backscatter intensity value approximately equivalent to a black and white photograph the elevation of the first backscatter return from the laser the highest thing hit and the elevation of the last return the lowest thing hit . For most pixels these two elevations will be the same but where bushes or trees are present some of the laser energy will be reflected from the top of the trees as a first hit return but some laser energy will also penetrate down to ground to produce the last hit return. The RTV program also provides a fourth derived product that is a color image combining the intensity image with hues that are derived from the elevations.

The three dimensional terrain visualizing method of a preferred embodiment of the present invention generates images of both rough and smooth areas in a terrain with increased detail. Terrain mapping data is processed as a series of pixels with pixels in relatively rough areas being depicted without modification while pixels in relatively smooth areas are effectively joined together to eliminate the small gaps in the surfaces. The preferred embodiment technique for identifying and joining relatively smooth areas as used herein is referred to herein as texture based segmentation. 

A preferred embodiment algorithm as described in detail below identifies relatively smooth local areas and warps the squares for these areas so that the edges meet and the surface is continuous. Areas that are truly rough such as trees are left as disjoint squares. Overall the effects on the squares before and after the algorithm is applied is minimal so that algorithm errors are not conspicuous.

In some preferred embodiments of present invention the texture based segmentation algorithm fuses the discrete rectangles to create a continuous surface where the map is relatively smooth. It works locally at each pixel within a 3 by 3 neighborhood of the pixel and determines whether the pixel is part of a relatively smooth surface by attempting to construct 4 lines through the pixel. If the two halves of a line have sufficiently similar slopes that is if the range difference from the center pixel and its neighbor on one side is close enough to the range difference to the neighbor on the other side then that line is determined to be valid. If 3 out of the 4 possible lines are valid then the center pixel is identified as locally smooth and a corner vertex in the center pixel is joined with the adjacent vertices in the neighbor pixels that contributed to the pixel being identified as locally smooth. 

Although the technique is described with respect to vertical imaging from the air the technique is just as applicable in the horizontal plane for ground based ladars primarily . The key distinction is that the information to be visualized come from a sensor like a ladar that makes discrete measurements of the scene. Synthesizing or modeling has been the most common approach to creating 3 D scenes. However ladars both airborne and vehicle carried can map huge areas cheaply and quickly and it is predicted that it will have even more applications in the future. For example by attaching a ladar to a vehicle one can obtain a 3 D map of a city in virtually perfect detail. This would provide a viewpoint not just from the viewpoint collected but anywhere within the perimeter of the area recorded. For example if one were to be interested in a particular corner the relevant 3 D image for the particular corner could be downloaded from a repository.

These and other aspects of the embodiments of the invention will be better appreciated and understood when considered in conjunction with the following description and the accompanying drawings. It should be understood however that the following descriptions while indicating preferred embodiments of the invention and numerous specific details thereof are given by way of illustration and not of limitation. Many changes and modifications may be made within the scope of the embodiments of the invention without departing from the spirit thereof and the embodiments of the invention include all such modifications.

A more complete appreciation of the invention will be readily obtained by reference to the following Description of the Preferred Embodiments and the accompanying drawings in which like numerals in different figures represent the same structures or elements. The representations in each of the figures are diagrammatic and no attempt is made to indicate actual scales or precise ratios. Proportional relationships are shown as approximates.

The embodiments of the invention and the various features and advantageous details thereof are explained more fully with reference to the non limiting embodiments that are illustrated in the accompanying drawings and detailed in the following description. It should be noted that the features illustrated in the drawings are not necessarily drawn to scale. Descriptions of well known components and processing techniques are omitted so as to not unnecessarily obscure the embodiments of the invention. The examples used herein are intended merely to facilitate an understanding of ways in which the embodiments of the invention may be practiced and to further enable those of skilled in the art to practice the embodiments of the invention. Accordingly the examples should not be construed as limiting the scope of the embodiments of the invention.

The terminology used herein is for the purpose of describing particular embodiments only and is not intended to limit the full scope of the invention. As used herein the singular forms a an and the are intended to include the plural forms as well unless the context clearly indicates otherwise. It will be further understood that the terms comprises and or comprising when used in this specification specify the presence of stated features integers steps operations elements and or components but do not preclude the presence or addition of one or more other features integers steps operations elements components and or groups thereof.

It will be understood that when an element such as an object layer region or substrate is referred to as being on or extending onto another element it can be directly on or extend directly onto the other element or intervening elements may also be present. In contrast when an element is referred to as being directly on or extending directly onto another element there are no intervening elements present. It will also be understood that when an element is referred to as being connected or coupled to another element it can be directly connected or coupled to the other element or intervening elements may be present. In contrast when an element is referred to as being directly connected or directly coupled to another element there are no intervening elements present.

It will be understood that although the terms first second etc. may be used herein to describe various elements components regions layers and or sections these elements components regions layers and or sections should not be limited by these terms. For example when referring first and second photons in a photon pair these terms are only used to distinguish one element component region layer or section from another region layer or section. Thus a first element component region layer or section discussed below could be termed a second element component region layer or section without departing from the teachings of the present invention.

Furthermore relative terms such as lower or bottom and upper or top may be used herein to describe one element s relationship to other elements as illustrated in the Figures. It will be understood that relative terms are intended to encompass different orientations of the device in addition to the orientation depicted in the Figures. For example if the device in the Figures is turned over elements described as being on the lower side of other elements would then be oriented on upper sides of the other elements. The exemplary term lower can therefore encompass both an orientation of lower and upper depending of the particular orientation of the figure. Similarly if the device in one of the figures is turned over elements described as below or beneath other elements would then be oriented above the other elements. The exemplary terms below or beneath can therefore encompass both an orientation of above and below. Furthermore the term outer may be used to refer to a surface and or layer that is farthest away from a substrate.

Embodiments of the present invention are described herein with reference to cross section illustrations that are schematic illustrations of idealized embodiments of the present invention. As such variations from the shapes of the illustrations as a result for example of manufacturing techniques and or tolerances are to be expected. Thus embodiments of the present invention should not be construed as limited to the particular shapes of regions illustrated herein but are to include deviations in shapes that result for example from manufacturing. For example a region or object illustrated as a rectangular will typically have tapered rounded or curved features. Thus the regions illustrated in the figures are schematic in nature and their shapes are not intended to illustrate the precise shape of a region of a device and are not intended to limit the scope of the present invention.

Unless otherwise defined all terms including technical and scientific terms used herein have the same meaning as commonly understood by one of ordinary skill in the art to which this invention belongs. It will be further understood that terms such as those defined in commonly used dictionaries should be interpreted as having a meaning that is consistent with their meaning in the context of the relevant art and will not be interpreted in an idealized or overly formal sense unless expressly so defined herein.

It will also be appreciated by those of skill in the art that references to a structure or feature that is disposed adjacent another feature may have portions that overlap or underlie the adjacent feature.

The following are definitions of some of the terms used in the detailed description. This list of terms is not exclusive additional terms may be defined in or may need to be understood in light of the specification. Although the words appear in capitals the word s need not have capitals for the definition s to apply.

COMPUTATIONAL DEVICE or PROCESSOR as used herein includes one or more general purpose computers CPUs multiprocessors microprocessors processors special purpose computers quantum computers special hardware devices or other machines that perform computations and or process the data used by the preferred embodiment of the present invention. A set of individual computational devices that are linked together through a communications mechanism such as a network connection would be treated as a single computational device.

COMPUTER READABLE MEDIUM as used herein means a storage medium such as a hard drive floppy drive CD ROM DVD USB drive flash memory read only memory or some other medium that a computer can readily process. Multiple media can be also be used.

COMPUTER READABLE PROGRAM CODE as used herein means machine code compilable source code or interpretable scripting code.

DISPLAYING as used herein means the rendering of a terrain or other image. Displaying can be performed for the benefit of people through various devices such as computer monitors stereoscopic displays plotters or printers. Displaying can also be performed for the benefit of other systems. Writing a series of coordinates to a file or to memory for used by another system that can use those coordinates to create an image is a method of displaying.

DRAWING as used in this application is the act of computing where something should go. For example drawing a line through two coordinates does not mean actually making a line visible but simply identifying the beginning and ending coordinates.

ELEVATION as used herein is a value indicating how far a point is from a base topography. Meters above sea level is an example of a unit of elevation based on the average sea level of the earth. Elevation does not have to be on a linear scale. For example logarithmic elevation values can be used such that incrementing an elevation in logarithmic scale might mean a doubling of the elevation in absolute terms. Elevation is relative to the base topography. A painting hung on a wall has an elevation when compared with the wall that is different from the elevation when compared with the floor. The base topography itself does not have to be flat. For example elevation data could be obtained by taking the height above sea level at every point along the equator. This would produce a circular base topography with elevations relative to the sea level along the equator.

As used herein NORTH EAST SOUTH WEST and combinations such as NORTHEAST and SOUTHWESTERN are directional words that relate to a reference point and or orientation. They do not mean absolute directional values. Thus a test pixel may be oriented so that its northern neighbor pixel is actually south of the test pixel in the real world. Moreover the directions are not necessary restricted to a particular plane and are equally applicable to a vertical plane. Similarly the nomenclature of top top left top right bottom middle left bottom left right and bottom right or the equivalent could be utilized with equal effect.

PIXEL as used herein as it relates to digital imaging is an elemental piece of information in an image. Pixels may be arranged in a 2 dimensional grid or 3 dimensional grid and may be represented using dots squares rectangles or some other geometric object. An image is formed from a plurality of pixels and each pixel is an element of an image. The intensity of each pixel is variable in color systems each pixel may have three or four components such as red green and blue or cyan magenta yellow and black. In the visualization technique of some preferred embodiments of the present invention a mapped surface is divided into equal sized squares. Each rendered surface datum or element within a square may be treated as a pixel. Other embodiments of the invention may or may not necessarily use square or uniform pixels. A pixel in the sense used in this application differs from a pixel in the sense of the smallest unit of display on a computer monitor. Computer monitor pixels typically only display the colors red green and blue where each color has a uniform intensity throughout the computer monitor pixel to the extent that the technology supports such uniformity. A pixel as used in this application can contain multiple colors and intensities. For example elevation data might be more course than image data for a terrain. The pixels used to visualize the elevations might therefore be larger than the colors and intensities representing the terrain image.

SENSOR as used herein means any device that can map terrain. Common terrain mapping devices include light detection and ranging LIDAR devices sometimes called laser radar LADAR and interferometric synthetic aperture radar sensors. Many other technologies can provide terrain mapping data. For example sonar can map the terrain of the ocean floor. Stereo photographs can also be used to map terrain.

TERRAIN as used herein means any set of surfaces for which elevation from a base topography can be acquired. This might be a city a forest the ocean floor a wall an extraterrestrial planet surface 3 D microscopic imagery or some other set of surfaces.

WARPING as used herein refers to the distortion of the perimeter of a pixel to the extent that it is joined with adjacent pixels. A pixel is warped when one of the vertices of the surface is moved to a new position and the surface is modified so that the surface touches all of the unaltered vertices and the altered vertex. There are many ways of accomplishing this task that are known in the art. One simple way is to generate triangular surfaces such that the triangles connect with all of the required vertices and to each other. Other variations are possible. Many tools readily available to the ordinary practitioner can readily warp a 3 D surface. shows the warping of the vertices and such that they make contact with each other.

In the ground and building rooftops are readily identifiable. With this particular technique the sides of the buildings also appear to be depicted because this rendering technique creates surfaces that connect data points even when there is a significant gap between the data points. In this case this depiction is misleading. Terrain data was collected from nearly overhead so there was little actual data on what was on the sides of buildings.

Creating surfaces to connect data points obscures small scale features such as the trees surrounding the buildings in . Tree tops are rendered but the surfaces connecting them to the ground make them look like hills or mountains. This severely undermines efforts to understand the nature of the depicted terrain.

Substantially similar information identifiable in is identifiable in . The ground and roof tops are both readily identifiable. The building sides are also identifiable but they are identifiable as an area where little information was available to the rendering engine. This approach avoids misleading the viewer into believing that more is known about the terrain than is actually known. The human mind quickly fills in the gaps and understands that there is some structure holding the roof tops but that the details of the structure are unknown. This conservative approach to rendering data stands out when rendering features that are not connected to the surface below such as bridges.

Tree tops may be much easier to interpret using this visualization approach. The sides of the trees are only depicted where data was actually sampled and the rough data remains rough. This makes the trees more closely resemble trees than the depiction of trees in .

While the visualization in is an improvement over the visualization in there are small gaps between pixels representing surfaces that are not quite horizontal. One building rooftop shows a number of these small gaps resembling a series of rows of short lines referenced as G in . These gaps clutter the image and make the contrast with bushes and trees less obvious.

The texture based segmentation algorithm of a preferred embodiment is designed to eliminate the small gaps in surfaces that are basically smooth i.e. without sharp changes in elevation. This algorithm described in detail below identifies locally relatively smooth areas and warps the squares for these areas so that the edges meet and the surface is continuous. Areas that are truly rough such as trees are left as disjoint squares. One important feature of this procedure is that there is only a small difference between the squares or pixels before and after the algorithm is applied so algorithm errors are not conspicuous. Thus the algorithm does not need to be extremely reliable and can be fast and simple. The algorithm may occasionally leave a gap in a relatively smooth area or occasionally joins several squares in a tree but such errors or gaps are almost imperceptible. The visual impression is to eliminate most of the small gaps in smooth surfaces making more striking the contrast between roads grass and buildings that are smooth and disjoint under sampled objects like trees. Thus the algorithm does not need to be extremely reliable and can be fast and simple.

For the map colors the hue is a function of the elevation with the highest elevations mapping to red then through yellow and green to blue which is the lowest map elevation. The display intensity is mapped to the intensity image from the ladar which is very useful in distinguishing features like roads that do not differ much in elevation from their surroundings but are much darker than the surrounding grass. The 3DEM RTV viewer illustrated in derives intensity from a shading algorithm which is particularly inappropriate for this scene since it mostly highlights the draping of the surface over the trees. Other viewers such as ESRI ArcGIS Explorer paint photographs as texture onto the elevation surface an approach that produces better quality images than the 3DEM RTV.

Alternative methods of selecting pixel hues and intensities may be employed without deviating from the scope of the claimed invention. For example photographic imagery can be matched with each pixel to provide significant detail. Metrics such as population density or land usage can also enhance terrain imagery.

Stereoscopic viewing is very useful in viewing any 3 D terrain map. However it is particularly useful for smaller scales with foliage and other objects that are under sampled. When we look through something like foliage the stereo is very helpful in sorting out the complex scenes with objects at different ranges.

Some embodiments of the invention may enable switching between the modes of displaying and hiding upper layers. This has many practical applications such as aiding in the identification of vehicles and other smooth objects that have been hidden under foliage.

Texture based segmentation involves iteratively reviewing a set of pixels to identify locally smooth areas. When a pixel is being examined to determine whether it is in a locally smooth area it is identified as the test pixel . When the pixels are laid out on a Cartesian coordinate system the neighbors of the test pixel can be identified as the northwestern neighbor the northern neighbor the northeastern neighbor the western neighbor the eastern neighbor the southwestern neighbor the southern neighbor and the southeastern neighbor . Similarly the coordinate system may reference a vertical plane as top middle bottom top left top right middle left middle right bottom left bottom right without departing from the scope of the present invention.

The purpose of these test lines is to help determine whether the test pixel is within a locally smooth area whether it should be joined to at least some of its neighbors to avoid rendering distracting gaps.

With texture based segmentation the slope m of the first half of the test line and the slope m of the second half of the test line are compared. If the two slopes are approximately equal then the test line is marked as smooth. If they are not approximately equal then the test line is marked as unsmooth. In some preferred embodiments of the invention whether they are approximately equal or not is determined by checking to see whether m m where is a threshold of similarity. Depending upon the circumstances and subject matter can be set to an exact value such as 0.3 it can be a value that is modifiable for each rendering or it can even be modifiable for different test pixels or different test lines. If the units of measurement in the source data are not uniform then the slope values or may need to be adjusted accordingly.

Terrain data comprising elevations at different coordinates in the terrain often does not have detailed information about steep parts of the terrain. Depicting smooth surfaces along these steep sections can produce misleading visualizations. To avoid this problem a maximum slope value m can be set. If either half of the test line has a slope that is steeper than m then the line is not marked as smooth. The value for mcan be set to an exact value such as 1.0 it can be a value that is modifiable for each rendering or it can even be modifiable for different test pixels or different test lines. If the units of measurement in the source data are not uniform then the slope values or mmay need to be adjusted accordingly.

In the scenario illustrated in the slope m for the first half of the test line is 1.0. If mis also 1.0 then mwould be considered level enough that the test line might be considered smooth. However the slope m for the second half of the test line is less than 1.0. Thus mwould be considered too steep and the test line would not be marked as smooth.

In some embodiments of the invention the number of smooth lines drawn through a given test pixel indicate whether the test pixel is in a locally smooth area. In some embodiments of the invention a smooth lines threshold value must be met for a pixel to be considered within a locally smooth area. can be set to an exact value such as 3 it can be a value that is modifiable for each rendering or it can even be modifiable for different test pixels. Different test lines can also contribute different weightings rather than simply contributing 1 to a smooth lines count.

The test line that ran through the northeastern neighbor did contribute to the determination that the test pixel was in a locally smooth area but in some embodiments of the invention it is not used in the process of joining the test pixel to its neighbor pixels and .

Other embodiments of the invention may employ a different process for joining test pixels with neighbor pixels. For example a weighted average may be used to identify the new location for the movable vertices or the test pixel s movable vertex may actually be fixed while the other movable vertices are adjusted.

It should be apparent to one of ordinary skill in the art that implementation details can vary without departing from the teachings of the invention. Flags can be used instead of sets. Hard coded steps can avoid use of certain sets. Tests for smoothness can include information from non adjacent pixels. Pixels can be joined using moving averages. Optimizations can be introduced to reduce computation time. Default data values can be used where no data exists. Other implementation details can be varied without departing from the scope of the claimed invention.

The terminology planar image as used in the following claims means an aerial photograph satellite photograph or a still photograph taken from ground level.

It should be emphasized that the above described embodiments are merely possible examples of implementations. Many variations and modifications may be made to the above described embodiments. All such modifications and variations are intended to be included herein within the scope of the disclosure and protected by the following claims. The term processor or computer as used herein includes multiprocessors computers supercomputers data processor laptops signal processors personal computers notebook computers and or any component which processes data. The term image generator as used herein includes a MEMS device or array which generate images and or any element or component including components within the MEMS structure which generate images which may be used as a screen or monitor. The abbreviation RF or rf is used for radio frequency or a radio frequency signal. The term subject as used herein means a person human or animal.

